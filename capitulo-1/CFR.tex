\section{Counterfactual Regret Minimization}

El objetivo de esta sección es presentar un algoritmo que permita encontrar un equilibrio de Nash en un juego en forma extensiva no determinista con información incompleta. Aunque todo juego en forma extensiva puede ser representado en forma normal, esto no es de mucho interés, pues la forma normal puede tener un tamaño exponencialmente más grande al tamaño del árbol. Se verá como el concepto de \textit{regret minimization} puede ser extendido a juegos secuenciales, sin necesidad de la forma normal explícita. Los conceptos, procedimientos y teoremas mostrados en esta sección, son presentados en \cite{bib:cfr}.

\subsection{Regret Minimization}

La primera definición clave es el \textit{regret}. Para esto, es necesario considerar jugar repetidamente un juego en forma extensiva. Sea $\sigma_i^t$ la estrategia usada por el jugador $i$ a tiempo $t$. La Definición \ref{def:average-overall-regret}, presenta el concepto de \textit{average overall regret}.

\begin{definition}
\label{def:average-overall-regret}
El \textbf{average overall regret} del jugador $i$ a tiempo $T$ es:
\begin{alignat}{1}
R_i^T = \frac{1}{T} \max_{\sigma^* in B_i} \sum_{t = 1}^T [u_i(\sigma_i^*, \sigma_{-i}^t) - u_i(\sigma^t)]
\end{alignat}
\end{definition}

Se denotará con $\bar{\sigma}_i^{T}$ la estrategia promedio del jugador $i$ del tiempo $1$ al tiempo $T$. En particular, para cada conjunto de información $I \in \mathcal{I}_i$ y para cada acción $a \in A(I)$ se define:
\begin{alignat}{1}
\bar{\sigma}_i^{T}(I)(a) = \frac{\sum_{t = 1}^T \pi^{\sigma^t_i}(I)\sigma^t_i(I)(a)}{\sum_{t = 1}^T \pi^{\sigma_i^t}(I)}
\end{alignat}

Esta estrategia es el promedio ponderado de las probabilidades $\sigma^t(I)(a)$ con respecto a que tan probable es alcanzar $I$ dado $\sigma_i^t$.

La relación entre el \textit{average overall regret} y el concepto de solución se muestra en el Teorema \ref{theo:regret-nash} \cite{bib:cfr}.
\begin{theorem}
\label{theo:regret-nash}
En un juego de $2$ jugadores de suma cero si el average overall regret a tiempo $T$ es menor que $\varepsilon$ entonces $\sigma^{-T}$ es un $2\varepsilon$-equilibrio de Nash
\end{theorem}
\begin{proof}

Se probará que la probabilidad de alcanzar $z$ bajo $\bar{\sigma}_i^T$ viene dada por el promedio de alcanzar $z$ en cada estrategia. Sean $h_1 \sqsubset h_2 \sqsubset h_3 \sqsubset ... \sqsubset h_m \sqsubset z$ todos los prefijos de $z$ correspondientes al jugador $i$, es decir $P(h_k) = i\ \forall k : 1 \leq k \leq m$ y sean $a_1, a_2, ..., a_m$ las acciones correspondientes en $z$ en cada historia respectiva. Luego:
\begin{alignat}{2}
\pi^{\bar{\sigma}_i^T}(z)\ &=\ \prod_{k = 1}^m \bar{\sigma}_i^T(I(h_k))(a_k) \\	&=\ \prod_{k = 1}^m \frac{\sum_{t = 1}^{T}  \pi^{\sigma_i^t}(I(h_k))\sigma^t_i(I(h_k))(a)}{\sum_{t = 1}^T \pi^{\sigma_i^t}(I(h_k))}
\end{alignat}

Por otra parte, note que $\pi^{\sigma_i^t}(I)\sigma_i^t(I(h_k))(a_k) = \pi^{\sigma_i^t}(I(h_{k+1}))$. Entonces:
\begin{alignat}{1}
	\pi^{\bar{\sigma}_i^T}(z)\ &=\ \frac{\sum_{t = 1}^T\pi^{\sigma_i^t}(I_m) \sigma_i^t(I_m)(a_m)}{\sum_{t = 1}^T \pi^{\sigma_i^t}(I_1)} \\
	&=\ \frac{\sum_{t = 1}^T \pi^{\sigma_i^t}(z)}{\sum_{t = 1}^T 1} \\
	&=\ \frac{1}{T} \sum_{t = 1}^T \pi^{\sigma_i^t}(z)
\end{alignat}

Además, se tiene que, para cualquier jugador $i$ y cualquier estrategia de $\sigma_i$:
\begin{alignat}{2}
\frac{1}{T} \sum_{t = 1}^T u_i(\sigma'_i, \sigma_{-i}^t)\ &=\ \frac{1}{T} \sum_{t = 1}^T \left( \sum_{z \in Z} \pi^{\sigma'_i}(z) \pi^{\sigma_{-i}^t}(z) \pi^c(z) \right) \\
	&=\ \sum_{z \in Z} u_i(z) \pi^{\sigma'_i}(z) \pi^c(z) \left( \frac{1}{T} \sum_{t = 1}^T \pi^{\sigma_{-i}^t}(z) \right) \\
	&=\ \sum_{z \in Z} u_i(z) \pi^{\sigma'_i}(z) \pi^{\bar{\sigma}_i^T}(z) \pi^c(z) \\
	&=\ u_i(\sigma'_i, \bar{\sigma}_{-i}^T)
\end{alignat}

Por otra parte, como $R_2^T \leq \varepsilon$, para todo $\sigma'_2 \in B_2$ se tiene que:
\begin{alignat}{2}
	& \frac{1}{T} \sum_{t = 1}^T[u_2(\sigma_1^t, \sigma'_2) - u_2(\sigma^t)]\  \leq\  \varepsilon\\
	\Rightarrow\ & \frac{1}{T} \sum_{t = 1}^T u_2(\sigma^t) + \varepsilon\  \geq\  \frac{1}{T} \sum_{t = 1}^T u_2(\sigma_1^t, \sigma'_2) \\
	\Rightarrow\ & \frac{1}{T} \sum_{t = 1}^T u_2(\sigma^t) + \varepsilon\ \geq\ u_2(\bar{\sigma}_1^T, \sigma'_2)
\end{alignat}

Luego, como se cumple para cualquier $\sigma'_2$, se cumple para $\sigma_2^t$ para $t = 1, 2, ..., T$, obteniendo:
\begin{alignat}{2}
\frac{1}{T} \sum_{t = 1}^T u_2(\sigma^t) + \varepsilon\ & \geq\ \frac{1}{T} \sum_{t = 1}^T u_2(\bar{\sigma}_1^T, \sigma_2^t) \\
&=\ \frac{1}{T} \sum_{t = 1}^T \sum_{z \in Z} u_2(z) \pi^{\bar{\sigma}_1^T}\pi^{\sigma_2^t}(z) \pi^c(z) \\
&=\  \sum_{z \in Z} u_2(z) \pi^{\bar{\sigma}_1^T} \pi^c(z) \left( \frac{1}{T} \sum_{t = 1}^T \pi^{\sigma_2^t}(z) \right)\\
&=\  \sum_{z \in Z} u_2(z) \pi^{\bar{\sigma}_1^T} \pi^{\bar{\sigma}_2^T} \pi^c(z) \\
&=\ u_2(\bar{\sigma}^T)
\end{alignat}

Como $\Gamma$ es un juego de suma cero, se tiene que $u_2 = -u_1(\sigma)$ para toda estrategia $\sigma$, luego:
\begin{alignat}{2}
	& \frac{1}{T} \sum_{t = 1}^T u_2(\sigma^t) + \varepsilon\ \geq\  u_2(\bar{\sigma}^T) \\
	\Rightarrow\ & \frac{1}{T} \sum_{t = 1}^T -u_1(\sigma^t) + \varepsilon\ \geq\  -u_1(\bar{\sigma}^T) \\
	\Rightarrow\ &  u_1(\bar{\sigma}^T)\ + \varepsilon\ \geq\   \frac{1}{T} \sum_{t = 1}^T u_1(\sigma^t)  \\
	\Rightarrow\ &  u_1(\bar{\sigma}^T)\ + 2\varepsilon\ \geq\   \frac{1}{T} \sum_{t = 1}^T u_1(\sigma^t) + \varepsilon
\end{alignat}

Por otra parte, como $R_i^t \leq \varepsilon$ se tiene que, para cualquier $\sigma'_1 \in B_1$:
\begin{alignat}{1}
	\frac{1}{T} \sum_{t = 1}^T u_1(\sigma^t) + \varepsilon\ \geq\ \frac{1}{T} \sum_{t = 1} u_1(\sigma'_1, \sigma_2^t)\ =\ u_1(\sigma'_1, \bar{\sigma}_2^T)
\end{alignat}

Luego, se obtiene que:
\begin{alignat}{1}
	& u_1(\bar{\sigma}^T)\ + 2\varepsilon\ \geq\   \frac{1}{T} \sum_{t = 1}^T u_1(\sigma^t) + \varepsilon\ \geq\ u_1(\sigma'_1, \bar{\sigma}_2^T)\\
	\Rightarrow\ & u_1(\bar{\sigma}^T)\ + 2\varepsilon\ \geq\ u_1(\sigma'_1, \bar{\sigma}_2^T)
\end{alignat}

Análogamente, se demuestra que:
\begin{alignat}{1}
	u_2(\bar{\sigma}^T)\ + 2\varepsilon\ \geq\ u_2( \bar{\sigma}_1^T, \sigma'_2)
\end{alignat}

Concluyendo que $\bar{\sigma}^T$ es un $2\varepsilon$-equilibrio correlacionado.
\end{proof}

Como consecuencia del teorema anterior se obtiene que un algoritmo que lleve el \textit{average overall regret} a cero, conducirá a un equilibrio de Nash. La idea fundamental del enfoque presentado a continuación, propuesto en \cite{bib:cfr}, consiste en descomponer el \textit{average overall regret} en un conjunto de términos aditivos de \textit{regret} que puedan ser minimizados independientemente. En particular es necesario introducir un par de nuevo concepto, la utilidad contrafactual (Definición \ref{def:utilidad-contrafactual}) y el \textit{regret} contrafactual inmediato (Definición \ref{def:regret-inmediato}).

\begin{definition}
\label{def:utilidad-contrafactual}
La \textbf{utilidad contrafactual} es la ganancia esperada dado que el conjunto $I$ es alcanzado y todos los jugadores juegan con la estrategia $\sigma$ con excepción del jugador $i$ que juega para alcanzar $I$. Formalmente, si $\pi^{\sigma}(h, h')$ es la probabilidad de ir de la historia $h$ a la historia $h'$, entonces:
\begin{alignat}{1}
u_i(\sigma, I) = \frac{\sum_{h \in H, z \in Z} \pi^{\sigma_{-i}(h)1} \pi^{\sigma}(h, z) u_i(z)}{\pi^{\sigma_{-i}}(I)}
\end{alignat}
\end{definition}

Para toda acción $a \in A(I)$, se define $\sigma|_{I \rightarrow a}$ como el perfil estratégico idéntico a $\sigma$ excepto que el jugador $i$ siempre elige $a$ en el conjunto de información $I$.

\begin{definition}
\label{def:regret-inmediato}
El \textbf{regret contrafactual inmediato} es:
\begin{alignat}{1}
R_{i, \text{imm}}^T (I) = \frac{1}{T} \max_{a \in A(I)} \sum_{t = 1}^T \pi^{\sigma_{-i}^t}(I)[u_i(\sigma^t|_{I \rightarrow a}, I) - u_i(\sigma^t, I)] ]
\end{alignat}
\end{definition}

Intuitivamente, el \textit{regret} contrafactual inmediato es el arrepentimiento del jugador $i$ en su decisión en el conjunto de información $I$, en términos de la utilidad contrafactual, con un término de ponderación adicional para la probabilidad contrafactual que $I$ alcanzaría en esa ronda si el jugador hubiera intentado hacer eso. Usualmente, es de mayor interés el \textit{regret} cuando es positivo, por lo que se define $R_{i, \text{imm}}^{T, +} (I) = max(R^T_{i, \text{imm}} (I), 0)$. Luego, se tiene el siguiente resultado:

\begin{theorem}
\begin{alignat}{1}
R_i^T \leq \sum_{I \in \mathcal{I}_i} R_{i, \text{imm}}^{T, +}(I)
\end{alignat}
\end{theorem}

Debido a que minimizar cada \textit{regret} contrafactual inmediato minimiza el \textit{average overall regret} arrepentimiento general promedio, es posible enfocarse en minimizar los primeros para obtener un equilibrio de Nash.

\subsection{Counterfactual Regret Minimization}

Antes de mostrar el algoritmo principal, denominado \textit{Counterfactual Regret Minimization} para los juegos en forma extensiva, es necesario introducir el algoritmo de \textit{Regret Matching} general. Este algoritmo puede ser descrito en un dominio donde hay un conjunto fijo de acciones $A$, una función $u^t : A \rightarrow \mathbb{R}$ y en cada ronda una distribución de probabilidad $p^t$ es elegida.

\begin{definition}
\label{def:regret}
El \textit{regret} de no haber elegido la acción $a \in A$ hasta tiempo $T$, se define como:
\begin{alignat}{1}
R_i^T(a) = \frac{1}{T} \sum_{t = 1}^T \left[u_i(a) - \sum_{a' \in A}p^t(a)u^t(a)\right]
\end{alignat}
\end{definition}

Se define $R^{t, +}(a) = \max(R^t(a), 0)$. Luego la distribución $p^{t+1}$ es elegida de la siguiente manera:
\begin{alignat}{1}
p^t(a) = 
\begin{cases}
\frac{R_i^{t, +}}{\sum_{a' \in A} R^{t, +}(a)}\ & \text{si }\ \sum_{a' \in A} R^{t, +}(a) > 0\\
\frac{1}{A}\ & \text{en otro caso}
\end{cases}
\end{alignat}

\begin{theorem}
Si $|u| = \max_{t \in \{1, 2, ... T\}} \max_{a, a' \in A}(u^t(a) - u^t(a'))$ entonces el regret del algoritmo de regret matching está acotado por:
\begin{alignat}{1}
max_{a \in A}R^t(a) \leq \frac{|u| \sqrt{|A|}}{\sqrt T}
\end{alignat}
\end{theorem}

Luego, el algoritmo de \textit{Counterfactual Regret Minimization} es una aplicación del algoritmo \textit{Regret Minimization} de forma independiente a cada conjunto de información. En particular, se mantiene, para cada $I \in \mathcal{I}_i$ y para todo $a \in A(I)$:
\begin{alignat}{1}
R_i^T(I, a) = \frac{1}{T} \sum_{t = 1}^T \pi^{\sigma^t_{-i}}(I)[u_i(\sigma^t|_{I \rightarrow a}, I) - u_i(\sigma^t, I)]
\end{alignat}

Se define $R_i^{T, +}(I, a) = \max(R_i^T(I, a), 0)$, luego a tiempo $T+1$ la estrategia elegida es:
\begin{alignat}{1}
\sigma_i^{T+1}(I)(a) =
\begin{cases}
\frac{R_i^{T, +}(I, a)}{\sum_{a' \in A(I) R_i^{T, +}(I, a')}}\ & \text{si } \sum_{a' \in A(I) R_i^{T, +}(I, a')} > 0 \\
\frac{1}{|A(I)|}\ & \text{en otro caso} 
\end{cases}
\end{alignat}

Este algoritmo consiste en seleccionar las acciones de forma proporcional a la cantidad del \textit{regret} contrafactual positivo de no haber elegido esa acción. Si ninguna de estas cantidades es positiva, entonces la acción se elige con una distribución uniforme. Luego, como cota de convergencia, se tiene el siguiente teorema:
\begin{theorem}
Si el jugador $i$ selecciona las acciones de acuerdo al procedimiento anterior, entonces $R^T_{i, \text{imm}}(I) \leq \Delta_{u, i} \frac{\sqrt{|A_i|}}{\sqrt{T}}$ y por lo tanto $R_i^T \leq \Delta_{u, i} |\mathcal{I}_i| \frac{|A_i|}{\sqrt T}$, donde $|A_i| = max_{h : P(h) = 1}{|A(h)|}$
\end{theorem}