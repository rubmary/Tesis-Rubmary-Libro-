\chapter{Counterfactual Regret Minimization}
\label{section:cfr}

El objetivo de esta sección es presentar un algoritmo que permita encontrar un equilibrio de Nash en un juego en forma extensiva no determinista con información incompleta. Aunque todo juego en forma extensiva puede ser representado en forma normal, esto no es de mucho interés, pues la forma normal puede tener un tamaño exponencialmente más grande al tamaño del árbol. Se verá como el concepto de \textit{regret minimization} puede ser extendido a juegos secuenciales, sin necesidad de la forma normal explícita. Los conceptos, procedimientos y teoremas mostrados en esta sección, son presentados en \cite{bib:cfr}.

\section{Regret Minimization}

La primera definición clave es el \textit{regret}. Para esto, es necesario considerar jugar repetidamente un juego en forma extensiva. Sea $\sigma_i^t$ la estrategia usada por el jugador $i$ a tiempo $t$. La Definición \ref{def:average-overall-regret}, presenta el concepto de \textit{average overall regret}.

\begin{definition}
\label{def:average-overall-regret}
El \textbf{average overall regret} del jugador $i$ a tiempo $T$ es:
\begin{alignat}{1}
R_i^T\ =\ \max_{\sigma^* \in B_i} \, \frac{1}{T}\, \sum_{t = 1}^T \, u_i(\sigma_i^*, \sigma_{-i}^t) - u_i(\sigma^t) \,.
\end{alignat}
\end{definition}

Se denotará con $\bar{\sigma}_i^{T}$ la estrategia promedio del jugador $i$ del tiempo $1$ al tiempo $T$. En particular, para cada conjunto de información $I \in \mathcal{I}_i$ y para cada acción $a \in A(I)$ se define:
\begin{alignat}{1}
\bar{\sigma}_i^{T}(I)(a)\ =\ \frac{\sum_{t = 1}^T \pi^{\sigma^t_i}(I)\sigma^t_i(I)(a)}{\sum_{t = 1}^T \pi^{\sigma_i^t}(I)} \,.
\end{alignat}

Esta estrategia es el promedio ponderado de las probabilidades $\sigma^t(I)(a)$ con respecto a que tan probable es alcanzar $I$ dado $\sigma_i^t$.

La relación entre el \textit{average overall regret} y el concepto de solución se muestra en el Teorema \ref{theo:regret-nash} \cite{bib:cfr}.
\begin{theorem}
\label{theo:regret-nash}
En un juego de $2$ jugadores de suma cero si el average overall regret a tiempo $T$ es menor que $\varepsilon$ entonces $\sigma^{-T}$ es un $2\varepsilon$-equilibrio de Nash.
\end{theorem}

Como consecuencia del teorema anterior se obtiene que un algoritmo que lleve el \textit{average overall regret} a cero, conducirá a un equilibrio de Nash. La idea fundamental del enfoque presentado a continuación, propuesto en \cite{bib:cfr}, consiste en descomponer el \textit{average overall regret} en un conjunto de términos aditivos de \textit{regret} que puedan ser minimizados independientemente. En particular es necesario introducir un par de nuevo concepto, la utilidad contrafactual (Definición \ref{def:utilidad-contrafactual}) y el \textit{regret} contrafactual inmediato (Definición \ref{def:regret-inmediato}).

\begin{definition}
\label{def:utilidad-contrafactual}
La \textbf{utilidad contrafactual} es la ganancia esperada dado que el conjunto $I$ es alcanzado y todos los jugadores juegan con la estrategia $\sigma$ con excepción del jugador $i$ que juega para alcanzar $I$. Formalmente, si $\pi^{\sigma}(h, h')$ es la probabilidad de ir de la historia $h$ a la historia $h'$, entonces:
\begin{alignat}{1}
u_i(\sigma, I)\ =\ \frac{\sum_{h \in I, z \in Z} \pi^{\sigma_{-i}(h)} \pi^{\sigma}(h, z) u_i(z)}{\pi^{\sigma_{-i}}(I)} \,.
\end{alignat}
\end{definition}

Para toda acción $a \in A(I)$, se define $\sigma|_{I \rightarrow a}$ como el perfil estratégico idéntico a $\sigma$ excepto que el jugador $i$ siempre elige $a$ en el conjunto de información $I$.

\begin{definition}
\label{def:regret-inmediato}
El \textbf{regret contrafactual inmediato} es:
\begin{alignat}{1}
R_{i, \text{imm}}^T(I)\ =\  \max_{a \in A(I)} \, \frac{1}{T} \, \sum_{t = 1}^T \, \pi^{\sigma_{-i}^t}(I) \, \bigl[ u_i(\sigma^t|_{I \rightarrow a}, I) - u_i(\sigma^t, I) \bigr]\,.
\end{alignat}
\end{definition}

Intuitivamente, el \textit{regret} contrafactual inmediato es el arrepentimiento del jugador $i$ en su decisión en el conjunto de información $I$, en términos de la utilidad contrafactual, con un término de ponderación adicional para la probabilidad contrafactual que $I$ alcanzaría en esa ronda si el jugador hubiera intentado hacer eso. Usualmente, es de mayor interés el \textit{regret} cuando es positivo, por lo que se define $R_{i, \text{imm}}^{T, +} (I) = max(R^T_{i, \text{imm}} (I), 0)$. Luego, se tiene el siguiente resultado:

\begin{theorem}
\begin{alignat}{1}
R_i^T\ \leq\ \sum_{I \in \mathcal{I}_i} R_{i, \text{imm}}^{T, +}(I) \,.
\end{alignat}
\end{theorem}

Debido a que minimizar cada \textit{regret} contrafactual inmediato minimiza el \textit{average overall regret} arrepentimiento general promedio, es posible enfocarse en minimizar los primeros para obtener un equilibrio de Nash.

\section{Counterfactual Regret Minimization}

Antes de mostrar el algoritmo principal, denominado \textit{Counterfactual Regret Minimization} para los juegos en forma extensiva, es necesario introducir el algoritmo de \textit{Regret Matching} general. Este algoritmo puede ser descrito en un dominio donde hay un conjunto fijo de acciones $A$, una función $u^t : A \rightarrow \mathbb{R}$ y en cada ronda una distribución de probabilidad $p^t$ es elegida.

\begin{definition}
\label{def:regret}
El \textit{regret} de no haber elegido la acción $a \in A$ hasta tiempo $T$, se define como:
\begin{alignat}{1}
R_i^T(a) = \frac{1}{T} \sum_{t = 1}^T \left[u_i(a) - \sum_{a' \in A}p^t(a)u^t(a)\right]
\end{alignat}
\end{definition}

Se define $R^{t, +}(a) = \max(R^t(a), 0)$. Luego la distribución $p^{t+1}$ es elegida de la siguiente manera:
\begin{alignat}{1}
p^t(a) = 
\begin{cases}
\frac{R_i^{t, +}}{\sum_{a' \in A} R^{t, +}(a)}\ & \text{si }\ \sum_{a' \in A} R^{t, +}(a) > 0\\
\frac{1}{|A|}\ & \text{en otro caso}
\end{cases}
\end{alignat}

\begin{theorem}
Si $|u| = \max_{t \in \{1, 2, ... T\}} \max_{a, a' \in A}(u^t(a) - u^t(a'))$ entonces el regret del algoritmo de regret matching está acotado por:
\begin{alignat}{1}
max_{a \in A}R^t(a)\ \leq\ \frac{|u| \sqrt{|A|}}{\sqrt T} \,.
\end{alignat}
\end{theorem}

Luego, el algoritmo de \textit{Counterfactual Regret Minimization} es una aplicación del algoritmo \textit{Regret Minimization} de forma independiente a cada conjunto de información. En particular, se mantiene, para cada $I \in \mathcal{I}_i$ y para todo $a \in A(I)$:
\begin{alignat}{1}
R_i^T(I, a)\ =\ \frac{1}{T} \sum_{t = 1}^T \pi^{\sigma^t_{-i}}(I)[u_i(\sigma^t|_{I \rightarrow a}, I) - u_i(\sigma^t, I)]
\end{alignat}

Se define $R_i^{T, +}(I, a) = \max(R_i^T(I, a), 0)$, luego a tiempo $T+1$ la estrategia elegida es:
\begin{alignat}{1}
\sigma_i^{T+1}(I)(a)\ =\
\begin{cases}
\frac{R_i^{T, +}(I, a)}{\sum_{a' \in A(I) R_i^{T, +}(I, a')}}\ & \text{si } \sum_{a' \in A(I) R_i^{T, +}(I, a')} > 0 \,, \\
\frac{1}{|A(I)|}\ & \text{en otro caso.} 
\end{cases}
\end{alignat}

Este algoritmo consiste en seleccionar las acciones de forma proporcional a la cantidad del \textit{regret} contrafactual positivo de no haber elegido esa acción. Si ninguna de estas cantidades es positiva, entonces la acción se elige con una distribución uniforme. Luego, como cota de convergencia, se tiene el siguiente teorema:
\begin{theorem}
Si el jugador $i$ selecciona las acciones de acuerdo al procedimiento anterior, entonces $R^T_{i, \text{imm}}(I) \leq \Delta_{u, i} \frac{\sqrt{|A_i|}}{\sqrt{T}}$ y por lo tanto $R_i^T \leq \Delta_{u, i} |\mathcal{I}_i| \frac{|A_i|}{\sqrt T}$, donde $|A_i| = max_{h : P(h) = 1}{|A(h)|}$.
\end{theorem}


\section{Monte Carlo Conterfactual Regret Minimization}

En la sección \ref{section:cfr} se explicó el algoritmo de CFR, utilizado para resolver juegos en forma extensiva. Sin embargo, en la versión presentada es necesario recorrer el árbol completo en cada iteración, esta versión suele conocerse como \textit{vanilla} CFR. En \cite{bib:montecarlo-cfr} se describe una familia general de algoritmos CFR (basados en muestreo) denominada \textbf{Monte Carlo Conterfactual Regret Minimization} (MCCRF), para evitar recorrer el árbol completo en cada iteración.

La idea general es restringir los estados terminales alcanzados en cada iteración, pero manteniendo el mismo valor esperado para la utilidad contrafactual. Dada la definición \ref{def:informacion-incompleta}, sea $\mathcal{Q} = \{Q_1, Q_2, ..., Q_r\}$, un conjunto de subconjuntos de $Z$ tal que su unión sea igual a $Z$. Cada uno de estos conjuntos será llamado un bloque. Sea $q_j > 0$ la probabilidad de considerar el bloque $Q_j$ para la iteración actual (donde $\sum_{j = 1}^r {q_j} = 1$). 
Sea $q(z) = \sum_{j | j \in Q_i}$, es decir, $q(z)$ es la probabilidad de considerar $z$ en la iteración actual. La utilidad contrafactual muestreada, cuando se actualiza el bloque $j$ es:
\begin{alignat}{1}
\tilde{u}_i(\sigma, I | j)\ =\ \sum_{h \in I, z \in Q_j} \frac{\pi^{\sigma_{-i}}(h) \pi^{\sigma}(h, z) u_i(z)}{q(z) \pi^{\sigma_{-i}}(I)}
\end{alignat}

\begin{lemma}
$E_{j \sim q_j} [\tilde{u}_i(\sigma, I | j)]\ =\ u_i(\sigma, I)$
\end{lemma}
\begin{proof}
\begin{alignat}{2}
    E_{j \sim q_j}[\tilde{u}_i(\sigma, I | j)] & = \sum_{j} {q_j u_i(\sigma, I)} \\
    & = \sum_{j} { q_j \frac{\sum_{h \in I, z \in Q_j} \pi^{\sigma_{-i}}(h) \pi^{\sigma}(h, z) u_i(z)}{q(z) \pi^{\sigma_{-i}}(I)}} \\ \label{eq:def-esperanza}
    & = \sum_{j} \sum_{ \substack{h \in I \\ z \in Q_j}} \frac{q_j \pi^{\sigma_{-i}}(h) \pi^{\sigma}(h, z) u_i(z)}{ q(z) \pi^{\sigma_{-i}}(I)} \\ \label{eq:reorder-sum-1}
    & = \sum_{ \substack{h \in I \\ z \in Z} } \sum_{j | z \in Q_j} \frac{q_j \pi^{\sigma_{-i}}(h) \pi^{\sigma}(h, z) u_i(z)}{ q(z) \pi^{\sigma_{-i}}(I)} \\ \label{eq:reorder-sum-2}
    & = \sum_{ \substack{h \in I \\ z \in Z} } \left(\frac{\sum_{j | z \in Q_j} q_j }{q(z)}\right) \frac{\pi^{\sigma_{-i}}(h) \pi^{\sigma}(h, z) u_i(z)}{\pi^{\sigma_{-i}}(I)} \\
    & = \sum_{ \substack{h \in I \\ z \in Z} } \frac{\pi^{\sigma_{-i}}(h) \pi^{\sigma}(h, z) u_i(z)}{\pi^{\sigma_{-i}}(I)}  = u_i(\sigma, I) \label{eq:lemma-final-eq}
\end{alignat}

La ecuación \ref{eq:def-esperanza} se obtiene de la definición de $\tilde{u}_i(\sigma, I | j)$. \ref{eq:reorder-sum-1} y \ref{eq:reorder-sum-2} se obtienen al reordenar las sumatorias y considerando que la unión de los bloques generan a $Z$. La ecuación \ref{eq:lemma-final-eq}
\end{proof}

Si se elige $\mathcal{Q} = {Z}$, es decir un único bloque con todas las historias terminales y $q_1 = 1$, la utilidad contrafactual es igual a la utilidad contrafactual muestreada y se obtiene el algoritmo \textit{vanilla} CFR. Si se eligen los bloques para incluir todas las historias terminales con la misma secuencia de acciones en los nodos de azar se obtiene el \textit{chance-sampled} CFR, siendo esta última versión la utilizada para estudiar los juegos presentados en este trabajo de grado. Se implementa el algoritmo como es detallado en \cite{bib:introductionCFR} que se presenta en el \textit{\textbf{apéndice X}}.


\section{Evaluación de las Estrategias y Explotabilidad}

Para evaluar la convergencia de los algoritmos y la estrategia obtenida se utilizaron las métricas de \textit{regret} y explotabilidad, respectivamente.

La explotabilidad se obtiene al calcular la mejor respuesta de la estrategia de cada jugador y sumar los resultados, como se explicó en la sección \textit{\textbf{Explicar explotabilidad en el capítulo 2}}. Sin embargo, la diferencia es que en los juegos de forma extensiva no se pueden listar todas las estrategias fácilmente como en los juegos en forma normal, ya que esta tarea es exponencial en el tamaño del árbol.

Para calcular la explotabilidad en estos juegos se utilizó el algoritmo propuesto en \cite{bib:thesis-marc-lanctot}, denominado \textit{Generalized Expectimax Best Response} (GEBR), descrito en el apéndice \textit{\textbf{X}}. La complejidad de este algoritmo es $\mathcal{O}(ND)$ donde $N$ es el número de nodos del árbol y $D$ es la profundidad del árbol. Note que el algoritmo tiene una alta complejidad, por lo que se usará únicamente para calcular la explotabilidad de la estrategia final.


\section{Detalles de implementación}
Los algoritmos y la representación de los juegos fueron implementados en el lenguaje de programación C++. Para la representación de los juegos se utilizó una clase abstracta llamada \textit{Game}, que recibe como template los tipos para el estado, las acciones, las propiedades, los conjunto de información y el Hash del juego específico.

Esta clase contiene las funciones virtuales necesarias para recorrer el árbol del juego de forma \textbf{implícita}, tales como: \textit{actions}, que retornan las acciones del juegos en el estado actual, \textit{update\_state}, que actualiza el estado del juego dada una acción a realizar, \textit{terminal\_state} que indica si un estado es terminal o no, \textit{utiliy} que retorna la utilidad en un estado terminal, entre otras. Los algoritmos CFR y GEBR utilizan esta clase abstracta en su implementación.

Para cada tipo de juego, se creó una clase derivada de la clase \textit{Game}, donde se implementaron las funciones según las reglas de cada juego. De esta forma se puede utilizar la misma implementación de los algoritmos para todos los juegos.

Cabe destacar que todos los juegos fueron representados mediantes árboles con la raíz como único nodo de azar. Algunos juegos tienen esta representación de forma natural, por ejemplo, el Kuhn Poker, ya que las cartas se reparten al inicio y luego se juega acorde a esa distribución, sin volver a introducir ninguna jugada aleatoria. Otros juegos pueden tener nodos de azar distintos a la raíz, sin embargo siempre es posible transformarlos a un árbol que represente el mismo juego donde todos los nodos de azar son condensados en la raíz y cada hijo de la raíz representa una elección por cada uno de los nodos de azar del árbol original. En esta representación se asume que todas las decisiones aleatorias se toman al inicio del juego.

La clase \textit{Game} y todos los algoritmos se implementados suponiendo la raíz como único nodo de azar del juego.



\section{Descripción de los juegos}
\label{section-description-juegos-forma-extensiva}

Para probar los algoritmos se implementaron tres tipos de juegos diferentes: \textit{One Card Poker} (OCP), \textit{Dudo}, un juego de dados, y una versión del juego de dominó para $2$ personas. La descripción detallada de las reglas de los juegos se describen en esta sección.

\subsection*{Juego de One-Card Poker}
One-Card Poker, abreviado OCP(N), es la versión generalizada del juego Kuhn Póker, explicado en la sección \ref{section:kuhn-poker}. En este juego, cada jugador recibe una carta de un mazo de $N$ cartas, y luego pueden apostar o retirarse según las mismas reglas del Kuhn Póker. Note que OCP(3) es equivalente al Kuhn Poker. El árbol de este juego tiene $9N(N-1)+1$ nodos (incluyendo el nodo inicial, que es el nodo de azar) y hay $4N$ conjuntos de información entre ambos jugadores. 

\subsection*{Juego de Dudo}
Dudo, también conocido como \textit{Bluff}, \textit{Liar's Dice} o Perudo, es un juego de dados y apuestas. Usualmente se juega entre $2$ y $6$ jugadores. Los jugadores se ubican en forma circular y cada uno de ellos tiene un número de dados. De forma simultánea, todos lanzan sus dados, cada jugador puede ver el resultado de sus propios dados, pero no puede ver el resultado de los dados de los otros jugadores. Una vez hecho esto, los jugadores empiezan a apostar sobre el número de veces que apareció una cara en específico en todos los dados que hay en la mesa.

Una apuesta consiste en decir $2$ números $(x, y)$, esto indica que el jugador apuesta que hay, al menos, $x$ dados cuyo resultado fue el número $y$. El primer jugador (que se elige previamente mediante el lanzamiento de $1$ dado o de alguna otra forma), realiza la primera apuesta y, en sentido horario, cada jugador puede hacer una apuesta más alta o decir \say{dudo} y retar al jugador anterior. Una apuesta es más alta que otra si el número de dados que se anuncian en la apuesta ($x$) es mayor, o si el número de dados es igual, pero la cara apostada ($y$) es mayor. Por ejemplo $(3, 1)$ es mayor que $(2, 5)$, y ambas apuestas son mayores que $(2, 3)$.

Por otra parte, si un jugador reta al jugador previo, se descubren todos los dados de todos los jugadores. Si la cantidad de dados con la cara $y$ es mayor o igual a $x$, donde $(x, y)$ fue la apuesta realizada por el jugador, el jugador que hizo el reto pierde un dado. En caso contrario, el jugador que hizo la apuesta pierde un dado. Luego, todos los jugadores lanzan sus dados nuevamente y una nueva ronda de apuestas empieza por el jugador que perdió la ronda anterior. Un jugador pierde cuando se queda sin dados, el ganador es el último jugador con al menos un dado restante. La figura \ref{fig:dudo} muestra una foto del juego, de \textit{Perudo}, una versión comercial de este juego, que está diseñada para $6$ jugadores, donde cada jugador empieza con $5$ dados. En la figura se observan los vasos que se utilizan para lanzar los dados y evitar que cada jugador vea los dados de los demás.

\begin{figure}[htb]
\caption[Juego Dudo]{Juego Dudo. Los vasos se utilizan para lanzar los dados y evitar que los oponentes vean el resultado}
\label{fig:dudo}
\centering
\includegraphics[width=0.6\textwidth]{figuras/dudo.jpg}
\end{figure}

En este trabajo de grado consideraremos este juego para $2$ jugadores únicamente. Dudo$(K, D_1, D_2)$ hará referencia a una única ronda de apuestas de $2$ jugadores, donde el primer jugador tiene $D_1$ dados, el segundo jugador tiene $D_2$ dados y cada dado tiene $K$ caras. El juego completo consiste en múltiples rondas, donde $D_1$ o $D_2$ disminuye en una unidad al finalizar cada ronda. Cuando uno de los jugadores pierde todos los dados obtiene una utilidad de $-1$, mientras que su oponente obtiene una utilidad de $1$. En este juego cada ronda se considerará un subjuego y se representará con un árbol independiente, donde los valores esperados para los juegos Dudo$(K, D_1 - 1, D_2)$ y Dudo$(K, D_1, D_2 - 1)$ se precalculan y se utilizan como utilidad para las hojas del árbol Dudo$(K, D_1, D_2)$. Note que, en el juego estándar, $K$ siempre tiene un valor de $6$.

Cuando el jugador $i$ lanza $D_i$ dados hay $\binom{D_i+K-1}{K-1}$ resultados posibles diferentes, ya que cada resultado puede ser representado con una tupla $(a_1, a_2, ..., a_k)$, donde $a_j$ representa el número de dados con la cara $j$, por lo que $\sum_j^K = D_i$ y $a_j \geq 0$. Por otra parte cada secuencia de apuestas puede ser representada por una secuencia binaria de longitud $K(D_1 + D_2)$, donde el $i$-ésimo bit es $1$ si la $i$-ésima secuancia más fuerte fue dicha durante la ronda y $0$ en caso contrario. Por ejemplo, si $D_1 = D_2 = 1$, las apuestas $(1, 1)-(1, 3)-(1, 6)-(2, 4)-(2, 5)-(1, 6)$ se representa con la secuencia binaria $101001000110$, por lo que hay $2^{K(D_1 + D_2)}$ secuencias diferentes. Cada secuencia pertenece a un jugador en específico, por lo que si $D_1 = D_2$, el número de conjuntos de información  es igual a  $\binom{D_i+K-1}{K-1}2^{K(D_1 + D_2)}$.

Para contar el número total de nodos, se puede considerar el lanzamiento de los dados de forma independiente, pues las secuencias posibles de apuestas no dependen del resultado de los dados. Por lo expuesto anteriormente el número posible de apuestas es igual a $2^{K(D_1+D_2)}$, pero después de cada secuencia siempre se puede decir \say{dudo}, salvo para la secuencia vacía. Luego el número total de nodos (incluyendo nodos terminales y no terminales) es igual a $\binom{D_1+K-1}{K-1}\binom{D_1+K-1}{K-1}(2^{K(D_1+D_2)+1}-1)+1$.

\subsection*{Juego de Dominó}
En este trabajo se utilizó una versión de este juego para $2$ jugadores. Al inicio del juego cada jugador toma una cantidad específica de piezas de forma aleatoria, las piezas restantes se dejan sin descubrir para ser usadas en turnos posteriores. Como en el juego tradicional de dominó, los jugadores juegan por turnos alternados (el primero jugador se elige de forma arbitraria), cada uno debe colocar una ficha válida acorde a las reglas \textit{estándares} en Venezuela del juego (ver apéndice \textit{\textbf{X}}). Si un jugador no puede colocar una ficha toma una ficha de las que no están descubiertas (si todavía hay disponibles), el jugador verifica si puede colocar la ficha tomada y en caso contrario pasa el turno y juega el oponente.

El juego termina cuando alguno de los jugadores usa todas las piezas o cuando ambos jugadores no pueden jugar ni tomar piezas nuevas, en este último caso se dice que el juego está bloqueado. El ganador es el jugador que se queda sin piezas o, en caso de bloqueo, el jugador que acumule menos puntos en todas las piezas que quedaron en su mano. La utilidad obtenida es el número de puntos que el jugador perdedor acumuló en las piezas que quedaron en su mano (con signo positivo para el jugador ganador y signo negativo para el perdedor). Cabe destacar que sólo se puede tomar una pieza o pasar, si no se puede realizar una jugada con la mano actual.

Usualmente se utilizan $28$ piezas, donde las piezas pueden tener entre $0$ y $6$ puntos en cada extremo, y cada jugador recibe $7$ piezas al inicio del juego. En este trabajo se parametriza el número máximo de puntos que puede tener una ficha, así como la cantidad de piezas repartidas inicialmente. De esta forma se hará referencia a Domino$(M, N)$ an un juego donde las piezas tienen entre $0$ y $M$ puntos (con un total de $(M+1)(M+2)/2$ piezas) y cada jugador recibe $N$ piezas al inicio del juego.

En este juego no es fácil calcular el tamaño del árbol y el número de conjuntos de información, principalmente porque las acciones posibles en un estados dependen tanto de la mano del jugador, como de las piezas en la mesa. En el Kuhn Póker simpre hay $2$ acciones posibles ${pasar, apostar}$ y en el Dudo las acciones disponibles dependen únicamente de la última apuesta y no dependen de los dados que tengan los jugadores. Así que se decidió estimar estos parámetros recorriendo el árbol del juego mediante DFS. La tabla \ref{tab:tree-domino}, muestra el número de nodos del árbol y el número de conjuntos de información por cada juego de dominó que se presenta.

\begin{table}[ht]
    \centering
    \begin{tabular}{c|r|r}
        & Conjutos de Información & Nodos \\ \hline
       Domino$(1, 1)$ & $3$ & $13$ \\
       Domino$(2, 2)$ & $441$   & $7321$ \\
       Domino$(3, 2)$ & $844437$   & $46534657$ \\
       Domino$(3, 3)$ & $1082290$   & $246760993$ \\ 
       Domino$(3, 4)$ & $902218$   & $1547645185$ \\ \hline
    \end{tabular}
    \caption{Número de nodos y conjuntos de Información en diferentes juegos de Dominó}
    \label{tab:tree-domino}
\end{table}


\section{Resultados experimentales}

Se crearon varias instancias de los $3$ juegos explicados en la sección \ref{section-description-juegos-forma-extensiva} con diferentes parámetros. Para cada instancia se utilizó el algoritmo de CFR y se iteró sobre el árbol durante $10$ (\textit{\textbf{número tentativo}}) horas (se excluye el tiempo que se calcula el regret ya que no forma parte del algoritmo y esto se hace únicamente para obtener las gráficas). Una vez terminado el tiempo asignado se calcula la mejor respuesta para cada jugador y la explotabilidad. Una instancia de un juego se considerará resuelta si la explotabilidad de la estrategia obtenida es menor que el $1\%$ de la mínima unidad de utilidad posible según cada juego.

La tabla \ref{tab:resultados-CFR} resume los resultados, donde $N$ representa el número de nodos del árbol $I$ el número de conjuntos de información, $u_{\sigma}$ el valor del juego usando la estrategia obtenida y $\varepsilon_{\sigma}$ la explotabilidad. También se agrega el número de iteraciones y la última columna indica si el juego fue resuelto o no, según lo establecido en el párrafo anterior.

\begin{table}[ht]
    \centering
    \begin{tabular}{l|r|r|r|r|r|c}
        Juego & $N$ & $I$ & Iteraciones & $u$ & $\varepsilon$ & Resuelto \\ \hline
        OCP$(3)$        &         55 &       12 & & & & \cmark \\
        OCP$(12)$       &       1.189 &      48 & & & & \cmark \\
        OCP$(50)$       &      22.051 &     200 & & & & \cmark \\
        OCP$(200)$      &     358.201 &     800 & & & & \cmark \\
        OCP$(1000)$     &   8.991.001 &   4.000 & & & & \cmark \\
        OCP$(4000)$     & 143.964.001 &  16.000 & & & & \cmark \\
        \hline
        Dudo$(3, 1, 1)$ &      $1144$ &      $192$ & & & & \cmark \\
        Dudo$(3, 2, 1)$ &     $18415$ &     $2304$ & & & & \cmark \\
        Dudo$(3, 1, 2)$ &     $18415$ &     $2304$ & & & & \cmark \\
        Dudo$(3, 2, 2)$ &    $294877$ &    $24576$ & & & & \cmark \\
        Dudo$(4, 1, 1)$ &      $8177$ &     $1024$ & & & & \cmark \\
        Dudo$(4, 2, 1)$ &    $327641$ &    $28672$ & & & & \cmark \\
        Dudo$(4, 1, 2)$ &    $327641$ &    $28672$ & & & & \cmark \\
        Dudo$(4, 2, 2)$ &  $13107101$ &   $655360$ & & & & \xmark \\
        Dudo$(5, 1, 1)$ &     $51176$ &     $5120$ & & & & \cmark \\
        Dudo$(5, 2, 1)$ &   $4915126$ &   $327680$ & & & & \cmark \\
        Dudo$(5, 1, 2)$ &       4.915.126 &   $327680$ & & & & \cmark \\
        Dudo$(5, 2, 2)$ &     471.858.976 & $15728640$ & & & & \xmark \\
        Dudo$(6, 1, 1)$ &         294.877 &    $24576$ & & & & \cmark \\
        Dudo$(6, 2, 1)$ &      66.060.163 &  $3538944$ & & & & \cmark \\
        Dudo$(6, 1, 2)$ &      66.060.163 &  $3538944$ & & & & \cmark \\
        Dudo$(6, 2, 2)$ &  14.797.504.071 &  $352321536$ & & & & \xmark \\
        \hline
        Domino$(2, 2)$ &     $441$ &       $7321$ & & & & \cmark \\
        Domino$(3, 2)$ &  $844437$ &   $46534657$ & & & & \cmark \\
        Domino$(3, 3)$ & $1082290$ &  $246760993$ & & & & \cmark \\
        Domino$(3, 4)$ &  $902218$ & $1547645185$ & & & & \cmark \\
        Domino$(4, 2)$ & & & & & & \xmark \\
        \hline
    \end{tabular}
    \caption{Resultados del algortimo CFR en los diferentes juegos}
    \label{tab:resultados-CFR}
\end{table}

La gráfica \textit{\textbf{Mostrar una o dos gráficas interesantes y decir algo al respecto}}. En el apéndice \textit{\textbf{X}} se pueden observar todas las gráficas del regret con respecto al número de iteraciones, se nota que el regret tiende a $0$ en todos los casos (\textit{\textbf{Agregar algún otro detalle interesante que se vea en las gráficas}})




