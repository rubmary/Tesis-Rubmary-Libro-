\chapter{Regret Matching}
\label{chapter:regret-matching}

En la sección~\ref{section:equilibrio-correlacionado} se introdujo el concepto de quilibrio correlacionado (Definición~\ref{def:equilibrio-correlacionado}). Además, se afirma que el conjungo de equilibrios correlacionado es un conjunto simple. Esto conlleva a la siguiente interrogante ?`Hay procedimientos adaptativos simples que permitan calcular un equilibrio correlacionado? A continuación se describen tres procedimientos, dos de los cuales llevan a equilibrios correlacionados \cite{bib:correlated-equilibrium}.

\subsection*{Procedimiento A: Regret condicional}

Sea $\Gamma$ un juego en forma normal el cual es jugado repetidamente a través del tiempo $t = 1, 2, \ldots $. 
Sea $h_t = (s^\tau)_{\tau = 1}^t \in \prod_{\tau = 1}^{t} S$ la historia del juego al inicio del tiempo $t+1$. El jugador $i \in N$ elige su estrategia con una distribución de probabilidad $p_{t+1}^i \in \Delta(S_i)$, definida de la siguiente manera.

Para cada par de estrategias $j, k \in S_i$, supongamos que el jugador $i$ remplaza la estrategia $j$ (cada vez que la jugó en el pasado) por la estrategia $k$. Luego, su ganancia a tiempo $1\leq \tau \leq t$ hubiera sido:
\begin{alignat}{1}
W_i^{\tau}(j,k)\ =\ 
\begin{cases}
u_i(k, s_{-i}^{\tau}) &\text{ si } s_i = j \,, \\
u_i(s^\tau) & \text{en otro caso.} 
\end{cases}
\end{alignat}
La diferencia resultante en el promedio de la función de pago, denotada con $D_i^t(j, k)$, para el jugador $i$ sería:
\begin{alignat}{1}
  D_i^t(j, k)\ 
    =\ \frac{1}{t} \sum_{\tau = 1}^{t} W_i^{\tau}(j, k) - \frac{1}{t} \sum_{\tau = 1}^{t} u_i(s^{\tau})\ 
	=\ \frac{1}{t} \sum_{\substack{1\leq \tau \leq t \\s^\tau_i = j}} u_i(k, s_{-i}^{\tau}) - u_i(s^{\tau}) \,.
\end{alignat}
Finalmente, definimos
\begin{alignat}{1}
\label{eq:regret}
R_i^t(j, k)\ =\ [D_i^t(j, k)]^+\ =\ \max(0, D_i^t(j, k)) \,.
\end{alignat}

La expresión \eqref{eq:regret} se puede interpretar como una medida de ``arrepentimiento'' del jugador $i$ de haber elegido la acción $j$ en vez de la acción $k$ en el pasado, y por lo tanto, dicha medida es denominada \textit{regret}.

Fijemos un número $\mu > 0$ suficientemente grande. Sea $j \in S_i$ la última estrategia jugada por el jugador $i$, es decir $j = s_i^t$. Luego, la distribución de probabilidad $p_{t+1}^i \in \Delta(S_i)$ usada por el jugador $i$ a tiempo $t+1$ es definida como:
\begin{alignat}{1}
\label{eq:proc-A}
  \begin{cases}
    p_{t+1}^i(k)\ :=\  \frac{1}{\mu} R_t^i(j, k) & \text{ si } k \neq j \,, \\
    p_{t+1}^i(j)\ :=\ 1 - \sum_{k \in S_i, k \neq j} p_{t+1}^i(k) \,.
  \end{cases}
\end{alignat}
La distribución inicial $p_{1}^i \in \Delta(S_i)$, a tiempo $t=1$, es elegida de forma arbitraria.

Para cada tiempo $t$, sea $z_t \in \Delta(S)$ la distribución empírica de las $N$-tuplas jugadas hasta tiempo $t$, es decir:
$z_t(s) := \frac{1}{t} |\{1\leq\tau \leq t : s^{\tau} = s \}|$. El siguiente teorema enuncia que el procedimiento arriba descrito produce un equilibrio correlacionado.

\begin{theorem}[\cite{bib:correlated-equilibrium}]
\label{theo:conv-proc-A}
Si cada jugador juega de acuerdo al procedimiento descrito por \eqref{eq:proc-A}, entonces la distribución empírica del juego $z_t$ converge (a.s.) cuando $t \rightarrow \infty$ al conjunto de equilibrios correlacionado del juego $\Gamma$.
\end{theorem}

Es importante destacar que $z_t$ no tiene que converger necesariamente a un punto equilibrio correlacionado. El Teorema~\ref{theo:conv-proc-A} es equivalente al siguiente enunciado: para todo $\varepsilon > 0$, existe un tiempo $T_0 = T_0(\varepsilon)$ tal que para todo $t \geq T_0$ podemos encontrar un equilibrio correlacionado $\psi_t$ que está distancia menor que $\varepsilon$ de $z_t$.

En el procedimiento descrito cada jugador tiene dos opciones en cada período: continuar jugando con la última estrategia, o cambiarla por otra estrategia cuyas probabilidades son proporcionales a cuanto mayor hubiese sido su ganancia acumulada si hubiese hecho ese cambio en el pasado. El procedimiento planteado es simple, tanto de entender y explicar, como de implementar. Además en cada período no sólo se elige la mejor respuesta, todas las respuestas mejores a la actual pueden ser escogidas con probabilidades que son proporcionales a sus ganancias aparentes (medidas por el \textit{regret}). Este tipo de procedimientos son llamados procedimientos de \textit{regret matching}. Por último, el procedimiento tiene inercia: la estrategia jugada previamente importa, siempre hay una probabilidad positiva de continuar jugando la misma estrategia, y más aún, sólo se cambiará de estrategia si hay una razón para hacerlo.

El \textit{regret} juega un papel importante para la elección de la siguiente distribución de probabilidad, lo cual conlleva a la siguiente pregunta: ?`Cuál es la relación entre los \textit{regrets} y el equilibrio correlacionado? Una condición necesaria y suficiente para que la distribución empírica converja al conjunto de equilibrio correlacionado es que todos los \textit{regrets} converjan a cero (Proposición \ref{prop:no-regret}).

\begin{theorem}
\label{prop:no-regret}
Sea $(s_t)_{t = 1, 2, ...}$ una secuencia de juegos de $\Gamma$.
Entonces, $R_i^t(j, k)$ converge a $0$ para cada $i$ y cada $j, k \in S_i$, con $j \neq k$, si y sólo si la secuencia de distribuciones empíricas $z_t$ converge al conjunto de equilibrio correlacionado.
\end{theorem}


\subsection*{Procedimiento B: Vector invariante de probabilidad}

Este procedimiento es una variación del anterior. Sin embargo, a tiempo $t+1$ las probabilidades de transición de la estrategia utilizada por el jugador $i$ son determinadas por la matriz estocástica (derecha) $M^i_t$ definida en \eqref{eq:proc-A}; i.e., $M^i_t(j,k)=\frac{1}{\mu}R^i_t(j,k)$ si $k\neq j$, y $M^i_t(j,j)=1-\frac{1}{\mu}\sum_{k\in S_i,k\neq j} R^i_t(j,k)$.

Considere un vector (fila) invariante de probabilidad $q^i_t$, donde $q^i_t\in \Delta(S_i)$, para la matriz $M^t$. Es decir, $q^i_t$ satisface $q^i_t \times M^i_t = q^i_t$ (dicho vector siempre existe):
\begin{alignat}{1}
  q^i_t(j)\ 
    =\ \sum_{k\in S_i} q^i_t(k) M^i_t(k,j)\ 
    =\ \bigg[\sum_{k \in S_i, k \neq j} q^i_t(k)\frac{1}{\mu}R^i_t(k,j)\bigg] + q_i^t(j)\biggl[1 - \frac{1}{\mu}\sum_{k \in S_i, k \neq j} R^i_t(j,k)\biggr]
\end{alignat}
para todo $j \in S_i$. Definamos $R_t^i(j, j) = 0$, luego:
\begin{alignat}{3}
  &
  & q^i_t(j)\ &=\ \biggl[\sum_{k \in S_i} q^i_t(k)\frac{1}{\mu}R^i_t(k,j)\biggr] + q^i_t(j)\biggl[1 - \sum_{k \in S_i} \frac{1}{\mu} R^i_t(j,k)\biggr] \\
  &\implies\quad
  &\mu q_t^i(j)\ &=\ \biggl[\sum_{k \in S_i}q^i_t(k)R^i_t(k,j)\biggr] + q^i_t(j)\biggl[\mu - \sum_{k \in S_i} R^i_t(j, k)\biggr] \\
  &\implies\quad
  &\mu q^i_t(j)\ & =\ \biggl[\sum_{k \in S_i}q^i_t(k)R^i_t(k,j)\biggr] + \mu q^i_t(j) - q^i_t(j)\sum_{k\in S_i} R^i_t(j,k) \,.
\end{alignat}
Por lo tanto,
\begin{alignat}{1}
\label{eq:proc-B}
q^i_t(j)\sum_{k \in S_i} R^i_t(j,k)\ =\ \sum_{k \in S_i} q_t^i(k)R_i^t(k,j) \,.
\end{alignat}

\begin{theorem}
Supongamos que a cada período $t+1$, el jugador $i$ elige las estrategias acorde a un vector de distribución de probabilidad $q_t^i$ que satisface \eqref{eq:proc-B}. Entonces, $R^i_t(j, k)$ converge a cero (a. s.) para todo $j, k \in S_i$ con $j \neq k$.
\end{theorem}

\subsection*{Procedimiento C: Regret incondicional}

El tercer procedimiento no conduce necesariamente a un equilibrio correlacionado. Sin embargo es considerado ``universalmente consistente'' (Definición \ref{def:proc-univ-consistente}). En este procedimiento, el pago promedio del jugador $i$, en el límite, no es peor a el pago si él hubiese jugado cualquier estrategia constante $k$, para todo $\tau \leq t$.

\begin{definition}[{\cite[p.~1139]{bib:correlated-equilibrium}}]
\label{def:proc-univ-consistente}
Un procedimiento adaptativo es \textbf{universalmente consistente} para el jugador $i$ si:
\begin{alignat}{1}
	\limsup_{t \rightarrow \infty } \left[ \max_{k \in S_i} \frac{1}{t} \sum_{\tau = 1}^{t} u_i(k, s_{-i}^{\tau}) - \frac{1}{t} \sum_{\tau = 1}^{t} u_i(s_{\tau}) \right]\ \leq\ 0\quad (a. s.)
\end{alignat}
\end{definition}
El procedimiento es definido a continuación. A tiempo $t$, definimos
\begin{alignat}{1}
D_i^t(k)\ &=\ \frac{1}{t} \sum_{\tau = 1}^{t} u_i(k, s_{-i}^{\tau}) - u_i(s_{\tau}) \,, \\
R_i^t(k)\ &=\ [D_i^t(k)]^+\ =\ \max(0, D_i^t(k)) \,.
\end{alignat}
Luego, la distribución de probabilidad a tiempo $t+1$, $p_{t+1}^i \in \Delta(S_i)$, es definida como sigue:
\begin{alignat}{1}
\label{eq:proc-C}
  p_{t+1}^i(k)\ =\ \frac{R_i^t(k)}{\sum_{k'\in S_i} R_i^t(k')}
\end{alignat}
si el denominador es positivo, y de forma arbitraria en caso contrario. Note que las probabilidades son elegidas de forma proporcional a $R_i^t(k)$ que será denominado \textit{regret} incondicional (en contraste al \textit{regret} condicional definido previamente).

\begin{theorem}
\label{theo:conv-proc-C}
El procedimiento adaptativo definido en \eqref{eq:proc-C} es universalmente consistente para el jugador $i$.
\end{theorem}


\section{Regret Matching y Equilibrio de Nash}

En el capítulo anterior se describieron procedimientos universalmente consistente, algunos de los cuales permiten obtener equilibrios correlacionados. Sin embargo, éstos no garantizan obtener un equilibrio de Nash, surgiendo la siguiente interrogante: ?`Bajo qué condiciones se puede garantizar que un procedimiento universalmente consistente conduzca a un equilibrio de Nash? El Teorema~\ref{theo:UC-EN} responde esta pregunta.

\begin{theorem}
\label{theo:UC-EN}
Sea $\Gamma$ un juego de dos jugadores de suma cero y sea $(s^t)_{t=1,2,..., T}$ una secuencia de juegos de $\Gamma$, tales que, para todo $s_i \in S_i$, para todo $i \in {1, 2}$:
\begin{alignat}{1}
\frac{1}{T}\sum_{t = 1}^{T}u_i(s_i, s_{-i}^t) - \frac{1}{T} \sum_{t = 1}^T u_i(s^t) \leq \varepsilon
\end{alignat}
para algún $\varepsilon > 0$. Sea $\bar{\sigma}^T = (\bar{\sigma_1}^T, \bar{\sigma_2}^T)$, donde:
\begin{alignat}{1}
\bar{\sigma}_i^T(s_i) = \frac{ |\{ 1 \leq T : s_i^t = s_i\}|}{T} = \frac{\#(s_i)}{T}
\end{alignat}
es decir, $\bar{\sigma}^T$, es la distribución empírica de probabilidad. Entonces $\bar{\sigma}^T$ es un $2\varepsilon$-equilibrio de Nash.
\end{theorem}

Entonces, en juegos de dos jugadores de suma cero, si un procedimiento es universalmente consistente, su distribución empírica llevará a un equilibrio de Nash. 

\section{Evaluación Empírica de Regret Matching}

Los algoritmos propuestos fueron probados en cuatro juegos diferentes en forma normal: \textit{matching pennies}, piedra, papel y tijera, ficha vs dominó y coronel blotto. Todos estos juegos son de dos jugadores para dos personas y, debido a que los algoritmos son universalmente consistentes, pueden ser utilizados para encontrar un equilibrio de Nash en cada uno de ellos. Además, es suficiente con definir el pago para del primer jugador para que el juego esté bien definido (ya que el pago del segundo jugador se obtiene al multiplicar por $-1$ el pago del primer jugador). 

Por otra parte, cualquier juego con estas características puede modelarse como un problema de programación lineal \cite[pp.~228-233]{bib:pl-chvatal} y resolverse mediante procedimientos destinados para esto. Esto nos permitirá encontrar, de forma teórica, un equilibrio de Nash para juegos suficientemente pequeños.

Cada uno de los juegos es descrito mediante sus reglas. Además, si el juego tiene un tamaño fijo y es lo suficientemente pequeños, mostraremos su matriz de pagos explícitamente, así como el problema de programación lineal asociado con alguna solución.

\subsection*{Juego de Matching Pennies}

En este juego cada jugador tiene una moneda y selecciona cara o sello de forma secreta. Si las elecciones son iguales gana el jugador $1$, en caso contrario gana el jugador $2$. La matriz de pagos de este juego se muestra en la Tabla \ref{table:pagos-matching-pennies}.

\begin{table}[ht]
\begin{center}
\caption[Tabla de pagos del juego matching pennies]{Tabla de pagos del juego \textit{matching pennies}}
\label{table:pagos-matching-pennies}
\begin{tabular}{ c | c | c |}
 & cara & sello  \\ \hline
 cara  &  1 & -1 \\ \hline
 sello & -1 &  1 \\ \hline
\end{tabular}
\end{center}
\end{table}

El problema de programación lineal asociado es el siguiente
\begin{equation}
\begin{array}{r r r r r r r r}
\max  & z &  & & & & &\\
\text{sujeto a} \\  
&   &   & x_1  & + & x_2 & = & 1 \\
& z & - & x_1  & + & x_2 & \leq & 0 \\
& z & + & x_1  & - & x_2 & \leq & 0 \\
&   &   & x_1, &   & x_2 & \geq & 0
\end{array}
\end{equation}

Cuya solución primal y dual vienen dada por:
\begin{alignat}{1}
(z^*, x_1^*, x_2^*) = \left(0, \frac{1}{2}, \frac{1}{2} \right) \\
(w^*, y^*_1, y^*_2) = \left(0, \frac{1}{2}, \frac{1}{2} \right)
\end{alignat}
 Luego el equilibrio de Nash se obtiene cuando ambos jugadores eligen cara o sello con probabilidad $\frac{1}{2}$ y el valor del juego es igual a $0$.

\subsection*{Juego de Piedra, Papel o Tijera}

Este juego es descrito en el Capítulo~\ref{chapter:forma-normal} y su matriz de pago se muestra en la Tabla \ref{table:pago-RPS}. El problema de programación lineal asociado es el siguiente:
\begin{equation}
\begin{array}{r r r r r r r r r r}
\max  & z &  & & & & &\\
\text{sujeto a} \\  
&   &   & x_1  & + & x_2  & + & x_3 & =    & 1 \\
& z &   &      & + & x_2  & - & x_3 & \leq & 0 \\
& z & - & x_1  &   &      & + & x_3 & \leq & 0 \\
& z & + & x_1  & - & x_2  &   &     & \leq & 0 \\
&   &   & x_1, &   & x_2, &   & x_3 & \geq & 0
\end{array}
\end{equation}

La solución primal y dual de este problema son
\begin{alignat}{1}
(z^*, x_1^*, x_2^*, x_3^*) = \left(0, \frac{1}{3}, \frac{1}{3}, \frac{1}{3}\right) \\
(w^*, y_1^*, y_2^*, y_3^*) = \left(0, \frac{1}{3}, \frac{1}{3}, \frac{1}{3}\right)
\end{alignat}
Luego, el equilibrio de Nash se obtiene cuando ambos jugadores eligen cada una de las acciones con probabilidad igual a $\frac{1}{3}$.

\subsection*{Juego de Ficha vs Dominó}

En este juego cada jugador tiene un tablero de tamaño $2\times 3$. El primer jugador tiene una ficha de dominó (que ocupa dos casillas con un lado en común) que puede colocar de $7$ formas diferentes, cada forma es mostrada en la Figura~\ref{fig:posiciones-domino}, con su respectiva etiqueta. El segundo jugador posee una ficha que ocupa una única casilla de su tablero y la ubica en una de las $6$ casillas, las cuales se numeran en la Figura~\ref{fig:posiciones}. Luego se superponen los tableros y si la ficha es cubierta por el dominó entonces el segundo jugador gana, en caso contrario gana el primer jugador \cite[p. 237]{bib:pl-chvatal}. La matriz de pagos de este juego se muestra en la Tabla \ref{table:pagos-domino}.

\begin{figure}
\caption{Posibles posiciones de la ficha de dominó}
\label{fig:posiciones-domino}
\centering
\includegraphics[width=1\textwidth]{figuras/posiciones-domino.png}
\end{figure}

\begin{figure}
\caption{Posibles posiciones de la ficha del segundo jugador}
\label{fig:posiciones}
\centering
\includegraphics[width=0.4\textwidth]{figuras/posiciones.png}
\end{figure}

\begin{table}
\begin{center}
\caption{Matriz de pagos del juego Ficha vs Dominó}
\label{table:pagos-domino}
\begin{tabular}{ c | c | c | c | c | c | c |}
  &  1 &  2 &  3 &  4 &  5 &  6 \\ \hline
A & -1 & -1 &  1 &  1 &  1 &  1 \\ \hline
B &  1 &  1 &  1 & -1 & -1 &  1 \\ \hline
C &  1 & -1 & -1 &  1 &  1 &  1 \\ \hline
D &  1 &  1 &  1 &  1 & -1 & -1 \\ \hline
E & -1 &  1 &  1 & -1 &  1 &  1 \\ \hline
F &  1 & -1 &  1 &  1 & -1 &  1 \\ \hline
G &  1 &  1 & -1 &  1 &  1 & -1 \\ \hline
\end{tabular}
\end{center}
\end{table}

El problema de programación lineal asociado es:
\begin{equation}
\begin{array}{r r r r r r r r r r r r r r r r r r}
max        &z& &   & &   & &   & &   & &   & &   & &   &      & \\
\text{sujeto a} \\  
	& & &x_1&+&x_2&+&x_3&+&x_4&+&x_5&+&x_6&+&x_7& =    &1 \\
    &z&+&x_1&-&x_2&-&x_3&-&x_4&+&x_5&-&x_6&-&x_7& \leq &0 \\
    &z&+&x_1&-&x_2&+&x_3&-&x_4&-&x_5&+&x_6&-&x_7& \leq &0 \\
    &z&-&x_1&-&x_2&+&x_3&-&x_4&-&x_5&-&x_6&+&x_7& \leq &0 \\
    &z&-&x_1&+&x_2&-&x_3&-&x_4&+&x_5&-&x_6&-&x_7& \leq &0 \\
    &z&-&x_1&+&x_2&-&x_3&+&x_4&-&x_5&+&x_6&-&x_7& \leq &0 \\
    &z&-&x_1&-&x_2&-&x_3&+&x_4&-&x_5&-&x_6&+&x_7& \leq &0 \\
    & & &x_1,&&x_2,&&x_3,&&x_4,&&x_5,&&x_6,&&x_7,&\geq &0 \\
\end{array}
\end{equation}

Este problema no tiene solución única (lo que implica que el juego no tiene un equilibrio de Nash único), una solución (primal y dual) viene dada por
\begin{alignat}{1}
(z^*, x^*_1, x^*_2, x^*_4, x^*_5, x^*_6, x^*_6, x^*_7) = \left(\frac{1}{3}, \frac{1}{3}, \frac{1}{3}, 0, 0, 0, 0, \frac{1}{3}\right) \\
(w^*, y^*_1, y^*_2, y^*_3,  y^*_4, y^*_5, y^*_6) = \left(\frac{1}{3}, \frac{1}{3}, 0, \frac{1}{3}, 0, \frac{1}{3}, 0\right)
\end{alignat}
Esta solución corresponde a la estrategia en la que el jugador $1$ elige las posiciones A, B y G con probabilidad $\frac{1}{3}$ cada una, y el jugador $2$ elige las posiciones $1$, $3$, y $5$ con probabilidad $\frac{1}{3}$ cada una.

\subsection*{Juego de Coronel Blotto}

En este juego cada uno de los jugadores tiene $S$ soldados en total que debe ubicar en $N$ campos de batallas. Cada soldado puede ser asignado a un único campo, pero cualquier número de soldados puede ser colocado en cualquier campo, incluyendo el cero. Un jugador obtiene un campo de batalla si asigna más soldados que su oponente en ese campo de batalla. El juego es ganado por el jugador que obtenga un mayor número de campos y su pago es igual a la diferencia entre el número de campos obtenidos por cada uno de los jugadores \cite{bib:blotto-game}.

Formalmente el juego puede ser descrito de la siguiente manera. Cada jugador debe elegir $N$ números enteros, digamos $(a_1, a_2, ..., a_N)$ y $(b_1, b_2, ..., b_N)$, para el jugador $1$ y $2$ respectivamente, tales que $a_1 + a_2 + ... + a_N = S$ y $b_1 + b_2 + ... + b_N = S$, con $N < S$, donde $a_i$ y $b_i$ es la cantidad de soldados ubicados el $i$-ésimo campo por el primer y segundo jugador, respectivamente. Para estas distribuciones, la ganancia del jugador $1$ viene dada por:
\begin{alignat}{1}
|\{ 1 \leq i \leq N : a_i > b_i\}| - |\{ 1 \leq i \leq N : a_i < b_i\}|
\end{alignat}

Este juego depende de dos parámetros: el número de soldados $S$ y el número de campos de batallas $N$, por lo que la matriz de pagos no es constante y por eso no es presentada como en los juegos anteriores. La matriz para de un juego con $S$ soldados y $N$ es una matriz cuadrada de tamaño $\binom{N+S-1}{S-1}$.

\section{Detalles de Implementación y Ejecución}

Los algoritmos fueron implementados en el lenguaje de programación C++, utilizando la librería estándar y una librería adicional llamada \textit{Eigen}, para factorizar matrices y resolver sistemas de ecuaciones.

Se implementó una clase para encontrar un equilibrio de Nash mediante el algoritmo de \textit{Regret Matching}. En cada iteración la actualización de las estrategias depende de cada procedimiento según las fórmulas propuestas en la sección anterior.

En el juego Coronel Blotto la matriz de pagos no tiene un tamaño fijo y además no es proporcionada de forma explícita, por lo que es necesario generarla dependiendo de los parámetros. Para esto se creó un programa que, dado el número de campos de batalla ($N$) y el número de soldados ($S$), genera todas las posibles distribuciones de cada uno de los jugadores mediante un algoritmo de \textit{backtracking} y calcula el pago para cada juego posible, obteniendo como salida del programa la matriz deseada. De esta forma se generó la matriz de pagos para este juego cuando $N = 3$ y $S = 5$.

Las ejecuciones de estos algoritmos se realizaron en una máquina personal, con las siguientes características:
\begin{itemize}[noitemsep]
    \item Procesador: Intel\textsuperscript{\textregistered} Core\textsuperscript{\texttrademark} i5-8250U CPU  \makeatletter{@} 1.60GHz
    \item 8CPUs
    \item 8GB de memoria RAM
    \item Sistema Operativo: Ubuntu 18.04.3 LTS
\end{itemize}


\section{Resultados Experimentales}

En esta sección se presenta un resumen de los resultados experimentales obtenidos utilizando el algoritmo \textit{Regret Matching} en los juegos descritos. Cada procedimiento fue probado $10$ veces por juego, finalizando cada corrida cuando se obtenía un regret máximo menor que $0.005$.

La tabla \ref{tab:resumen-resultados-RM} muestra un resumen de los resultados. En esta tabla se muestra, por cada juego, el tamaño de la matriz de pagos, el valor teórico del juego ($u_t$), el valor del juego utilizando la estrategia obtenida en la última corrida del algoritmo ($u_e$) y la explotabilidad ($\varepsilon_{\sigma}$). Las dos últimas métricas se presentan para cada uno de los procedimientos (A, B y C). En todos los casos se observa que la utilidad esperada para las estrategias obtenidas es cercana al valor del juego, además, se obtuvo una explotabilidad menor o igual que $0.011$, por lo que las estrategias obtenidas representan buenas aproximaciones al equilibrio de Nash en cada juego.

\begin{table}[htpb]
    \centering
    \begin{tabular}{l|r|r r r|r r r}
        &  & \multicolumn{3}{|c}{$u_e$} & \multicolumn{3}{|c}{$\varepsilon_{\sigma}$}  \\ \hline
        Juegos & $u_t$ & \multicolumn{1}{|c}{A} & \multicolumn{1}{c}{B} & \multicolumn{1}{c|}{C}
          & \multicolumn{1}{|c}{A} & \multicolumn{1}{c}{B} & \multicolumn{1}{c}{C} \\ \hline
        Matching Pennies
            & $0$ & $0$ & $0$ & $0$ & $0.006$ & $0.006$ & $0.008$ \\
        Piedra, Papel o Tijera
            & $0$ & $-0.000012$ & $0.000004$ & $0.000022$ & $0.006$ & $0.01$ & $0.009$ \\
        Ficha vs. Dominó
            & $\frac{1}{3}$ & $0.333$ & $0.334$ & $0.334$ & $0.01$ & $0.007$ & $0.004$ \\
        Coronel Blotto
            & $0$ & $0.000219$ & $-0.000502$ & $0.000024$ & $0.01$ & $0.011$ & $0.009$ \\
        \hline
    \end{tabular}
    \caption{Resumen de los resultados y evaluación de las estrategias obtenidas usando el algoritmo de Regret Matching en juegos en forma normal}
    \label{tab:resumen-resultados-RM}
\end{table}

Para evaluar la convergencia se midió el tiempo necesario para alcanzar el regret desea y el número de iteraciones, en la Tabla \ref{tab:resumen-regret-tiempo-RM} se presenta el tiempo ($T$), el número de iteraciones ($I$) y el tiempo por iteración ($T/I$) obtenido para cada uno de los juegos para cada procedimiento. Estos resultados son el promedio de las $10$ corridas realizadas por juego y procedimiento. Además, se crearon gráficas del regret por iteración para observar como disminuye a medida que corre el algoritmo, la  Figura  \ref{fig:regret-blotto} muestra las gráficas para el juego Coronel Blotto y los $3$ procedimientos. Estas gráficas son mostradadas con una escala logarítmica en el eje $x$ para apreciar mejor los resultados. En el Apéndice A se muestran las tablas detalladas con los resultados en cada una de las corridas y las gráficas de cada juego con los $3$ procedimientos.

\begin{figure}[htpb!]
    \caption{Gráficas del regret con respecto al número de iteraciones del juego Coronel Blotto}
    \label{fig:regret-blotto}
    \centering
    \includegraphics[width=0.58\textwidth]{graficas/coronel-blotto/procedimiento-A.png}
    \includegraphics[width=0.58\textwidth]{graficas/coronel-blotto/procedimiento-B.png}
    \includegraphics[width=0.58\textwidth]{graficas/coronel-blotto/procedimiento-C.png}
\end{figure}

\begin{table}[htpb]
    \centering
    \begin{tabular}{l | c |r r r}
        Juegos & Proc. & $T$ & $I$ & $T/I$ \\ \hline
        \multirow{3}{*}{Matching Pennies}
            & A & $10.276$ & $3892550.4$ & $2.64 {\times} 10^{-06}$ \\
            & B &  $0.777$ &   $25616.6$ & $3.03 {\times} 10^{-05}$ \\
            & C &  $0.042$ &   $16260.5$ & $2.58 {\times} 10^{-06}$ \\ \hline
        \multirow{3}{*}{Piedra, Papel o Tijera}
            & A & $12.198$ & $4519054.1$ & $2.70 {\times} 10^{-06}$ \\
            & B &  $0.345$ &    $6601.3$ & $5.23 {\times} 10^{-05}$ \\
            & C &  $0.049$ &   $19321.1$ & $2.54 {\times} 10^{-06}$ \\ \hline
        \multirow{3}{*}{Ficha vs. Dominó}
            & A & $319.179$ & $108319272.4$ & $2.95 {\times} 10^{-06}$ \\
            & B &  $11.275$ &     $75250.2$ & $1.50 {\times} 10^{-04}$ \\
            & C &   $0.237$ &     $84318.5$ & $2.81 {\times} 10^{-06}$ \\ \hline
        \multirow{3}{*}{Coronel Blotto}
            & A & $875.533$ & $190222305.3$ & $4.60 {\times} 10^{-06}$ \\
            & B &  $79.358$ &     $66378.4$ & $1.20 {\times} 10^{-03}$ \\
            & C &   $0.166$ &     $48613.5$ & $3.41 {\times} 10^{-06}$ \\ \hline
    \end{tabular}
    \caption{Resumen de los resultados y evaluación de las estrategias obtenidas usando el algoritmo de Regret Matching en juegos en forma normal}
    \label{tab:resumen-regret-tiempo-RM}
\end{table}

A continuación se se analiza el desempeño de los procedimientos, comparándolos entre sí, observando la rapidez de convergencia de cada uno de ellos.

\subsection{Complejidad de cada iteración}

Los procedimientos cambian en la forma en que se elige la siguiente estrategia en cada iteración. En los procedimientos A y B se utiliza un regret condicional, en el que se mide el \textit{arrepentimiento} de cambiar una estrategia por otra en específica. Esta métrica se debe mantener a lo largo de todas las iteraciones, por lo que cada iteración necesita memoria adicional de complejidad $\mathcal{O}(N^2 + M^2)$, donde $N$ y $M$ es el número de acciones posibles para el jugador $1$ y $2$, respectivamente. En el procedimiento C se utiliza únicamente el regret incondicional, por lo que la cantidad de memoria adicional es del orden $\mathcal{O}(N + M)$.

Con respecto a la complejidad de tiempo se tiene que los procedimientos de regret condicional e incondicional (A y C), son lineales al número de acciones. Sin embargo, en el procedimiento B es necesario resolver un sistema de ecuaciones lineales para elegir cada estrategia nueva, del tamaño del número de acciones del jugador respectivo, obteniendo que la complejidad total es $\mathcal{O}(N^3 + M^3)$. La Tabla \ref{tab:complejidades-iteraciones} muestra un resumen de la complejidad es tiempo y memoria adicional.

\begin{table}[ht]
    \centering
    \begin{tabular}{c|c|c}
         Procedimiento & Memoria & Tiempo  \\ \hline
         A & $\mathcal{O}(N^2 + M^2)$ & $\mathcal{O}(N + M)$ \\ 
         B & $\mathcal{O}(N^2 + M^2)$ & $\mathcal{O}(N^3 + M^3)$ \\
         C & $\mathcal{O}(N + M)$     & $\mathcal{O}(N + M)$ \\ \hline
    \end{tabular}
    \caption{Complejidad por iteración de cada uno de los procedimientos}
    \label{tab:complejidades-iteraciones}
\end{table}

Por lo anterior, se observa que la velocidad de las iteraciones del procedimiento que calcula el vector invariante de probabilidad es más lenta en todos los casos, estando uno o dos órdenes de magnitud por encima, según el tamaño de la matriz. Por lo que, si la matriz es sumamente grande, el segundo método sería el menos adecuado.

\subsection{Número de iteraciones}

En la Tabla \ref{tab:resumen-regret-tiempo-RM} se puede observar un resumen de las iteraciones promedio de los tres procedimientos en cada uno de los juegos. Se nota que el procedimiento A, regret incondicional es el que necesita muchas más iteraciones para converger. Con respecto a los procedimientos B y C, se observa que en algunos casos el promedio en el procedimiento B fue menor y en otros el promedio del procedimiento C. También es importante destacar que en el juego de piedra, papel o tijera se tienen varios casos donde se obtiene la convergencia en menos de $10$ iteraciones (ver apéndice A), esos son casos donde se obtiene el equilibrio de Nash de forma exacta en pocas iteraciones.

\subsection{Tiempo transcurrido}

Observando el tiempo promedio de los procedimientos en la tabla \ref{tab:resumen-regret-tiempo-RM}, se nota que el procedimiento A es el que emplea más tiempo en todos los casos, esto ocurre porque necesita muchas más iteraciones que los otros dos procedimientos. Por otra parte el procedimiento C es también más rápido que el procedimiento B, ya que la complejidad en cada iteración para resolver el sistema de ecuaciones enlentece el tiempo total necesario, incluso, si la matriz es muy grande este procedimiento podría ser más lento que el procedimiento A y no sería factible.

Aunque el procedimiento donde se aplica regret matching al regret incondicional (procedimiento C), es el más sencillo de implementar y el más rápido en converger, este procedimiento tiene una desventaja con respecto a los otros dos. Al utilizar el regret condicional, los dos primeros procedimientos garantizan que el regret condicional tiende a cero para cualquier par de estrategias de cada jugador y por lo tanto, conducen siempre a un equilibrio correlacionado. El tercer procedimiento sólo minimiza el regret incondicional y por lo tanto, si el juego es de más de dos jugadores o no es de suma cero, entonces ya no se puede garantizar que se encontrará alguna solución del juego.


