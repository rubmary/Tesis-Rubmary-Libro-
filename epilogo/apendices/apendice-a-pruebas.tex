\chapter{Pruebas}
\label{apex:chapter:pruebas}

\makeatletter
\newtheorem*{rep@theorem}{\rep@title}
\newcommand{\newreptheorem}[2]{
    \newenvironment{rep#1}[1]
    {
        \def\rep@title{#2 \ref{##1}}
        \begin{rep@theorem}
    }
     {
        \end{rep@theorem}
     }
}
\makeatother

\newreptheorem{theorem}{Teorema}


\section*{Capítulo I}

\begin{reptheorem}{theo:ganancia-esperada}
La ganancia esperada $u_i(\sigma)$ del jugador $i$ dado el perfil estratégico $\sigma$ satisface:
\begin{alignat}{1}
u_i(\sigma)\ =\ \sum_{s_i\in S_i} \sigma_i(s_i) \sum_{s_{-i}\in S_{-i}} \sigma_{-i}(s_{-i}) u_i(s_i,s_{-i}) \,.
\end{alignat}
\end{reptheorem}

\begin{proof}
Partiendo de la Definición \ref{def:ganancia-esperada} se obtiene
\begin{alignat}{1}
	u_i(\sigma)\  &=\ \sum_{s \in S} u_i(s) \sigma_i(s_i) \sigma_{-i}(s_{-i})\ =\ \sum_{s_i \in S_i} \sum_{s_{-i} \in S_{-i}} u_i(s_i, s_{-i}) \sigma_i(s_i) \sigma_{-i}(s_{-i}) \\
	&=\ \sum_{s_i\in S_i} \sigma_i(s_i) \sum_{s_{-i}\in S_{-i}} \sigma_{-i}(s_{-i}) u_i(s_i,s_{-i})\,.
\end{alignat}
\end{proof}

\begin{lemma}
\label{lemma:2}
Sea $\sigma^*_i$ una estrategia mixta para el jugador $i$ que es mejor respuesta a $\sigma_{-i}$, y sea $x\in S_i$ una estrategia pura para el jugador $i$. Entonces, para toda estrategia pura $y\in S_i$ diferente de $x$,
\begin{alignat}{1}
  \sigma^*_i(x) \sum_{s_{-i}} u_i(x,s_{-i}) \sigma_{-i}(s_{-i})\ \geq\ \sigma^*_i(x) \sum_{s_{-i}} u_i(y,s_{-i}) \sigma_{-i}(s_{-i}) \,.
\end{alignat}
\end{lemma}

\begin{proof}
Considere la estrategia mixta $\sigma'_i$ definida por:
\begin{alignat}{1}
	\sigma'_{i}(s_i)\ =\  
	\begin{cases}
		0 &  \text{si } s_i = x \\
		\sigma^*_i(x) + \sigma^*_i(y) & \text{si } s_i = y \\
		\sigma^*_i(s_i) & \text{en otro caso} \,. 
	\end{cases}
\end{alignat}
Utilizando el Teorema~\ref{theo:ganancia-esperada} y el hecho que $\sigma^*_i$ es mejor respuesta a $\sigma_{-i}$:
\begin{alignat}{1}
  u_i(\sigma^*_i, \sigma_{-i})\ 
    &\geq\ u_i(\sigma'_i, \sigma_{-i}) \\
    &=\ \sum_{z \in S_i} \sigma'_i(z) \sum_{s_{-i}} u_i(z,s_{-i}) \sigma_{-i}(s_{-i}) \\
    &=\ \sum_{z\neq x} \sigma^*_i(z) \sum_{s_{-i}} u_i(z,s_{-i}) \sigma_{-i}(s_{-i}) + \sigma^*_i(x)\sum_{s_{-i}} u_i(y,s_{-i})\sigma_{-i}(s_{-i}) \,.
\end{alignat}
Por el Teorema~\ref{theo:ganancia-esperada},
$u_i(\sigma^*_i, \sigma_{-i})=\sum_{z \in S_i} \sigma^*_i(z) \sum_{s_{-i}} u_i(z,s_{-i}) \sigma_{-i}(s_{-i})$. Entonces,
\begin{alignat}{1}
  \label{eq:ineq-ganancias}
  \sigma^*_i(x) \sum_{s_{-i}} u_i(x,s_{-i}) \sigma_{-i}(s_{-i})\
    &\geq\ \sigma^*_i(x)\sum_{s_{-i}} u_i(y,s_{-i}) \sigma_{-i}(s_{-i}) \,.
\end{alignat}
\end{proof}


\begin{reptheorem}{theo:mejor-respuesta}
Sea $\sigma^*_i$ una estrategia mixta para el jugador $i$ que es mejor respuesta a $\sigma_{-i}$. Cualquier estrategia mixta $\sigma_i$ para el jugador $i$ cuyo soporte sea un subconjunto del soporte de $\sigma^*_i$ es también una mejor respuesta a $\sigma_{-i}$.
\end{reptheorem}

\begin{proof}
Sea $x \in S_i$ una \emph{estrategia pura} perteneciente al soporte de $\sigma^*_i$, y sea $y \in S_i$ una estrategia pura \emph{diferente} de $x$. 

Por el Lema~\ref{lemma:2},
\begin{alignat}{1}
  \sum_{s_{-i}} u_i(x,s_{-i}) \sigma_{-i}(s_{-i})\ \geq\  \sum_{s_{-i}} u_i(y,s_{-i}) \sigma_{-i}(s_{-i}) \,.
\end{alignat}

En particular, si $x$ y $x'$ son distintos, y ambos pertenecen al soporte de $\sigma_i$,
\begin{alignat}{1}
\sum_{s_{-i}} u_i(x,s_{-i}) \sigma_{-i}(s_{-i})\ =\ \sum_{s_{-i}} u_i(x',s_{-i}) \sigma_{-i}(s_{-i})\ =\ C \,,
\end{alignat}
donde $C$ es una constante que s\'olo depende de $\sigma_{-i}$.
Luego, para cualquier estrategia $\sigma_i$, tal que $support(\sigma_i) \subseteq support(\sigma^*_i)$, se tiene:
\begin{alignat}{1}
u_i(\sigma_i, \sigma_{-i})\ &=\ \sum_{x \in S_i} \sigma_i(x) \sum_{s_{-i}} u_i(x,s_{-i}) \sigma_{-i}(s_{-i})\ =\ \sum_{x \in S_i} \sigma_i(x) C\ =\ C \,.
\end{alignat}

Luego, $u_i(\sigma^*_i,\sigma_{-i})=C$, y $\sigma_i$ es también mejor respuesta a $\sigma_{-i}$.
\end{proof}

\begin{reptheorem}{theo:nash-correlacionado}
Si $\sigma$ es un equilibrio de Nash, entonces $\sigma$ es un equilibrio correlacionado.
\end{reptheorem}

\begin{proof}
Sea $\sigma$ un equilibrio de Nash, sean $x,y\in S_i$ estrategias puras distintas cualesquiera para el jugador $i$, y sea $\sigma'_i$ una estrategia mixta cualquiera para el jugador $i$. Por el Lema~\ref{lemma:2},
\begin{alignat}{1}
  \sigma_i(x) \sum_{s_{-i}} u_i(x,s_{-i}) \sigma_{-i}(s_{-i})\ \geq\  \sigma_i(x)\sum_{s_{-i}} u_i(y,s_{-i}) \sigma_{-i}(s_{-i}) \,.
\end{alignat}

Es decir,
\begin{alignat}{1}
  0\ \leq\ \sigma_i(x) \sum_{s_{-i}} \sigma_{-i}(s_{-i}) [u_i(x,s_{-i}) - u_i(y,s_{-i})]\ =\ \sum_{s_{-i}} \sigma(x,s_{-i}) [u_i(x,s_{-i}) - u_i(y,s_{-i})] \,.
\end{alignat}
Luego, $\sigma$ es un equilibrio correlacionado.
\end{proof}

\begin{reptheorem}{theo:correlacionado-nash}
Sea $\psi\in\Delta(S)$ un equilibrio correlacionado. Si $\psi$ se factoriza como $\psi=\prod_{i\in N} \sigma_i$ donde $\{\sigma_i\}_{i\in N}$ es un conjunto de estrategias mixtas para cada jugador (i.e., $\psi(s)=\prod_{i \in N} \sigma_i(s_i)$ para todo $s\in S$), entonces $\psi$ es un equilibrio de Nash.
\end{reptheorem}

\begin{proof}
Sea $\psi= \prod_{i \in N} \sigma_i$ un equilibrio correlacionado en forma factorizada. Se debe mostrar que para cualquier jugador $i$ y estrategia mixta $\sigma'_i$ para el jugador $i$, se cumple $u_i(\sigma) \geq u_i(\sigma'_i, \sigma_{-i})$.

Sean $x$ y $y$ estrategias puras para el jugador $i$.
Como $\sigma$ es un equilibrio correlacionado,
\begin{alignat}{1}
\label{eq:1:theo:correlacionado-nash}
0\ \leq\ \sigma_i(x) \sum_{s_{-i}} \sigma_{-i}(s_{-i})[u_i(x, s_{-i}) - u_i(y, s_{-i})] \,.
\end{alignat}

Al sumar sobre $x\in S_i$ se obtiene, 
\begin{alignat}{2}
\label{eq:2:theo:correlacionado-nash}
0\ \leq\ \sum_{x\in S_i} \sum_{s_{-i}} \sigma(x,s_{-i}) [u_i(x, s_{-i}) - u_i(y, s_{-i})]\ =\ \sum_s \sigma(s) [u_i(s) - u_i(y, s_{-i})] \,.
\end{alignat}

Si $x^* \in S_i$ es tal que $\sigma_i(x^*)>0$, se obtiene de \eqref{eq:1:theo:correlacionado-nash} al multiplicar por $\sigma'_i(y)$ y sumar sobre $y\in S_i$:
\begin{alignat}{1}
\label{eq:3:theo:correlacionado-nash}
\sum_{y \in S_i} \sigma'_i(y) \sum_{s_{-i}} \sigma_{-i} (s_{-i}) [u_i(x^*, s_{-i}) - u_i(y, s_{-i})]\ =\ \sum_{s} \sigma'(s) [u_i(x^*, s_{-i}) - u_i(s)]\ \geq\ 0 \,,
\end{alignat}
donde $\sigma'$ denota la estrategia $\sigma'=(\sigma'_i,\sigma_{-i})$. 

Al sumar \eqref{eq:2:theo:correlacionado-nash} y
\eqref{eq:3:theo:correlacionado-nash}, se obtiene que para cualquier $y$ y $x^*$ tal que $\sigma_i(x^*)>0$:
\begin{alignat}{1}
\label{eq:4:theo:correlacionado-nash}
\sum_{s \in S} u_i(s) [\sigma(s) - \sigma'(s)] - \sum_{s \in S} \sigma(s)u_i(y, s_{-i}) + \sum_{s \in S} \sigma'(s) u_i(x^*, s_{-i})\ \geq\ 0\ \,.
\end{alignat}
Por otra parte, note que:
\begin{alignat}{1}
  \sum_{s \in S} \sigma(s) u_i(x^*,s_{-i}) - \sum_{s \in S} &\sigma'(s) u_i(x^*,s_{-i}) \\
    &\qquad=\ \sum_{s_{-i}} u_i(x^*,s_{-i}) \sigma_{-i}(s_{-i}) \sum_{z\in S_i} [\sigma_i(z) - \sigma'_i(z)] \\
    &\qquad=\ \sum_{s_{-i}} u_i(x^*,s_{-i}) \sigma_{-i}(s_{-i}) \biggl[ \sum_{z\in S_i} \sigma_i(z) - \sum_{z\in S_i} \sigma'_i(z) \biggr] \\
    &\qquad=\ 0 \,.
\end{alignat}
Luego, al tomar $y=x^*$ en \eqref{eq:4:theo:correlacionado-nash},
\begin{alignat}{1}
 \sum_{s \in S} u_i(s) [\sigma(s) - \sigma'(s)]\ =\ \sum_{s \in S} u_i(s)\sigma(s) - \sum_{s \in S} u_i(s)\sigma'(s)\ =\ u_i(\sigma) - u_i(\sigma'_i, \sigma_{-i})\ \geq\ 0 \,.
\end{alignat}
Como $\sigma'_i$ es una estrategia cualquiera para el jugador $i$, $\sigma$ es un equilibrio de Nash.
\end{proof}

\begin{reptheorem}{theo:correlacionado-linealidad}
Sean $\sigma$ y $\sigma'$ dos equilibrios correlacionados, y $\alpha$ un número real en $(0,1)$. Entonces, la distribución $\alpha\sigma + (1-\alpha)\sigma'$ es un equilibrio correlacionado.
\end{reptheorem}

\begin{proof}
Como $\sigma$ y $\sigma'$ son equilibrios correlacionados y $\alpha, 1 - \alpha \in (0, 1)$ se cumple que para cualesquiera $x$ e $y$:
\begin{alignat}{2}
 \alpha \sum_{s_{-i} \in S_{-i}} \sigma(x, s_{-i})[u_i(x, s_{-i}) - u_i(y, s_{-i})]\ & \geq\ 0 & \ \text{ y } \\
 (1 - \alpha) \sum_{s_{-i} \in S_{-i}} \sigma'(x, s_{-i})[u_i(x, s_{-i}) - u_i(y, s_{-i})]\ & \geq\ 0 \,.
\end{alignat}

Sumando las ecuaciones anteriores y factorizando se obtiene:
\begin{alignat}{1}
 \sum _{s_{-i} \in S_{-i}} [ \alpha \sigma(x, s_{-i}) +  (1 - \alpha) \sigma'(x, s_{-i})][u_i(x, s_{-i}) - u_i(y, s_{-i})]\ \geq\ 0 \,.
\end{alignat}

Concluyendo que $\alpha \sigma +  (1 - \alpha) \sigma'$ es un equilibrio correlacionado.
\end{proof}

\section*{Capítulo II}

\begin{reptheorem}{theo:comportamiento-a-mixta}
Dado un juego en forma extensa y un jugador $i$, tal que: si $h' \sqsubset h$ y $P(h') = P(h) = i$, entonces $I(h') \neq I(h)$. Luego, para cualquier estrategia de comportamiento $\sigma^b_i \in B^i$, la estrategia mixta $\sigma^m_i$ dada por:
\begin{alignat}{1}
\sigma^m_i(s_i) := \prod_{I_i \in \mathcal{I}_i} \sigma^b_i(I_i)(s_i(I_i)) \label{eq-apex:comportamiento-a-mixta}
\end{alignat}
es equivalente a la estrategia $\sigma^b_i$.
\end{reptheorem}

\begin{proof}
Se quiere probar que para todo $z \in Z$, se tiene que $\pi^{\sigma^m_i}(z) = \pi^{\sigma^b_i}(z)$. 
Para cualquier estrategia se denotará con $\sigma_i(s)$ la probabilidad de elegir la estrategia $s_i$ bajo $\sigma_i$. Además, la probabilidad de elegir una estrategia $s_i$ bajo la estrategia $\sigma^b_i$ es exactamente el lado derecho de la Ecuación \ref{eq:comportamiento-a-mixta}, la cual, por definición es la probabilidad de elegir $s_i$ bajo $\sigma^m_i$. Luego se tiene que $\sigma_i^b(s_i) = \sigma_i^m(s_i)$ para cualquier estrategia pura $s_i \in S_i$.

Por otra parte, como ninguna historia atraviesa más de una vez el mismo conjunto de información, se tiene que para cualquier estrategia $\sigma_i$ (mixta o de comportamiento):

\begin{alignat}{1}
\label{eq:definicion-mixta}
\pi^{\sigma_i}(z)\ =\ \sum_{\substack{s_i \in S_i \\  z \text{ es} \text{alcanzable} \\ \text{por } s_i} } \sigma_i(s_i) \,.
\end{alignat}

Luego, $\pi^{\sigma_i^b}(z) = \pi^{\sigma_i^m}(z)$ para todo $z \in Z$, obteniendo el resultado deseado. 
\end{proof}


\begin{reptheorem}{theo:mixta-a-comportamiento}
Dado un juego finito de $N$ personas en el que el jugador $i$ tiene ``perfect recall''. Entonces, para cada estrategia mixta $\sigma^m_i \in \Delta(S_i)$ del jugador $i$, existe una estrategia de comportamiento $\sigma^b_i \in B^i$, equivalente a $\sigma^m_i$.
\end{reptheorem}

\begin{proof}
Se denotará por $\pi^{\sigma_i}(I_i, a)$ la probabilidad, bajo $\sigma_i$, que $I_i$ sea alcanzable y se elija la acción $a$. De forma más general se denotará con $\pi^{\sigma_i}(I_i, a_1, a_2, ..., a_k)$ la probabilidad de que $I_i$ sea alcanzable y que luego el jugador $i$ elija las acciones $a_1, a_2, ... a_k$. Luego se elige la siguiente estrategia de comportamiento:
\begin{alignat}{1}
\sigma_i^b(I_i)(a)\ &=\ P[\text{se elija}\ a\ \text{bajo}\ \sigma^m_i | I_i\ \text{es alcanzable bajo}\ \sigma^m_i] \\
&=\ \frac{P[I_i\ \text{sea alcanzable bajo}\ \sigma^m_i\ \text{y se elija la opción}\ a\ \text{bajo}\ \sigma^m_i]}{ P[I_i\ \text{es alcanzable bajo}\ \sigma^m_i] } \\
&=\ \frac{\pi^{\sigma^m_i}(I_i, a)}{\pi^{\sigma^m_i}(I_i)} \label{eq:mixta-a-comportamiento} \,.
\end{alignat}
en caso que $\pi^{\sigma^m_i}(I_i) > 0$ y de forma arbitraria en caso contrario.

Se demostrará que $\pi^{\sigma^b_i}(z) = \pi^{\sigma^m_i}(z)$, cuando $\pi^{\sigma^m_i}(z) > 0$.  Dado $z \in Z $, sean $a_1, a_2, ..., a_k$ las acciones elegidas por el jugador $i$ (en ese orden), y sean $I^1_i, I^2_i, ..., I^k_i$ los conjuntos de información respectivos. Note que $\pi^{\sigma^m_i}(I^j_i, a^j_i) = \pi^{\sigma^m_i}(I^{j+1}_i)$, luego:
\begin{alignat}{1}
\pi^{\sigma^b_i}(z)\ &=\ \prod_{j = 1}^k \sigma^b_i(I^j_i)(a^j_i)\ =\ \prod _{j = 1}^k \frac{\pi^{\sigma^m_i}(I^j_i, a_j)}{\pi^{\sigma^m_i}(I^j_i)}\ =\ \frac{\pi^{\sigma^m_i}(I^k_i, a_k)}{\pi^{\sigma^m_i}(I^1_i)}\ =\ \pi^{\sigma^m_i}(I^k_i, a_k) \,.
\end{alignat}

Además, usando inducción, se obtiene que para cualquier $k' < k$ se tiene:
\begin{alignat}{1}
\pi^{\sigma^m_i}(I^k_i, a_k)\ =\ \pi^{\sigma^m_i} (I^{k'}_i, a_{k'}, a_{k'+1}, ..., a_{k})
\end{alignat}

Entonces
\begin{alignat}{1}
\pi^{\sigma^m_i}(I^k_i, a_k)\ =\ \pi^{\sigma^m_i}(I^1_i, a_1, a_2, ..., a_k) = \pi^{\sigma^m_i}(z)
\end{alignat}

Obteniendo $\pi^{\sigma^b_i}(z) = \pi^{\sigma^m_i}(z)$, que era lo que se quería demostrar.
\end{proof}

\section*{Capítulo III}

\begin{reptheorem}{theo:cota-ganancia-esperada}
Sea $\sigma^* = (\sigma^*_1, \sigma^*_2)$ un equilibrio de Nash de un juego de dos jugadores de suma cero, tal que $u_1(\sigma) = u$. Entonces $u_i(\sigma^*) \leq u_i(\sigma^*_i, \sigma_{-i})$, para cualquier estrategia $\sigma_{-i}$.  
\end{reptheorem}

\begin{proof}
Como $\sigma^*$ es un equilibrio de Nash, $\sigma^*_{-i}$ es mejor respuesta a $\sigma^*_i$ y por lo tanto, para cualquier estrategia $\sigma_{-i}$ se obtiene que:
\begin{alignat}{2}
    & u_{-i}(\sigma^*) = u_{-i}(\sigma^*_i, \sigma^*_{-i}) \geq u_{-i}(\sigma^*_i, \sigma_{-i}) \\
    \label{eq:juego-suma-cero}
    \implies\ & -u_i(\sigma^*) \geq -u_i(\sigma^*_i, \sigma_{-i}) \\
    \label{eq:multiplicar-1}
    \implies\ & u_i(\sigma^*) \leq u_i(\sigma^*_i, \sigma_{-i})
\end{alignat}

La inecuación~\ref{eq:juego-suma-cero} se obtiene al estar en un juego para dos jugadores de suma cero y la inecuación~\ref{eq:multiplicar-1} se obtiene al multiplicar por menos y cambiar la orientación de la desigualdad.
\end{proof}

\begin{reptheorem}{theo:EN-intercambiabilidad}
Sean $\sigma = (\sigma_1, \sigma_2)$ y $\sigma' = (\sigma'_1, \sigma'_2)$ equilibrios de Nash en un juego de dos jugadores con suma cero. Entonces $\sigma'' = (\sigma_1, \sigma'_2)$ y $\sigma''' = (\sigma'_1, \sigma_2)$ son también equilibrios de Nash. Además, $u_i(\sigma) = u_i(\sigma') = u_i(\sigma'') = u_i(\sigma''')$, para $i \in \{1, 2\}$.
\end{reptheorem}

\begin{proof}

Note que:
\begin{itemize}
    \item $u_i(\sigma_i, \sigma_{-i}) \leq u_i(\sigma_i, \sigma'_{-i})$ ocurre porque $\sigma$ es un equilibrio de Nash y el Teorema~\ref{theo:cota-ganancia-esperada}.
    \item $u_i(\sigma_i, \sigma'_{-i}) \leq u_i(\sigma'_i, \sigma'_{-i})$ ocurre porque $\sigma'$ es un equilibrio de Nash y entonces $\sigma'_i$ es mejor respuesta a $\sigma'_{-i}$.
    \item  $u_i(\sigma'_i, \sigma'_{-i}) \leq u_i(\sigma'_i, \sigma_{-i})$ ocurre porque $\sigma'$ es un equilibrio de Nash y el Teorema~\ref{theo:cota-ganancia-esperada}.
    \item $u_i(\sigma'_i, \sigma_{-i}) \leq u_i(\sigma_i, \sigma_{-i})$ ocurre porque $\sigma$ es un equilibrio de Nash y entonces $\sigma_i$ es mejor respuesta a $\sigma_{-i}$.
\end{itemize}

De lo anterior se obtiene que:
\begin{alignat}{1}
\label{eq:desigualdes-EN}
u_i(\sigma_i, \sigma_{-i})\ \leq\ u_i(\sigma_i, \sigma'_{-i})\ \leq\ u_i(\sigma'_i, \sigma'_{-i})\ \leq\ u_i(\sigma'_i, \sigma_{-i})\ \leq\ u_i(\sigma_i, \sigma_{-i}) \,.
\end{alignat}

% La primera desigualdad, $u_i(\sigma_i, \sigma_{-i}) \leq u_i(\sigma_i, \sigma'_{-i})$, ocurre porque $\sigma$ es un equilibrio de Nash y el Teorema~\ref{theo:cota-ganancia-esperada}. La segunda desigualdad, $u_i(\sigma_i, \sigma'_{-i}) \leq u_i(\sigma'_i, \sigma'_{-i})$, ocurre porque $\sigma'$ es un equilibrio de Nash y entonces $\sigma'_i$ es mejor respuesta a $\sigma'_{-i}$. La tercera desigualdad,  $u_i(\sigma'_i, \sigma'_{-i}) \leq u_i(\sigma'_i, \sigma_{-i})$ ocurre porque $\sigma'$ es un equilibrio de Nash y el Teorema~\ref{theo:cota-ganancia-esperada}. La última desigualdad, $u_i(\sigma'_i, \sigma_{-i}) \leq u_i(\sigma_i, \sigma_{-i})$, ocurre porque $\sigma$ es un equilibrio de Nash y entonces $\sigma_i$ es mejor respuesta a $\sigma_{-i}$.

Luego, todas las desigualdades en \ref{eq:desigualdes-EN} se cumplen como igualdad. Es decir $u_i(\sigma) = u_i(\sigma') = u_i(\sigma'') = u_i(\sigma''')$. Además $u_i(\sigma_i, \sigma'_{-i}) = u_i(\sigma'_i, \sigma'_{-i}) \geq u_i(\sigma''_i. \sigma'_{-i})$ y $u_i(\sigma'_i, \sigma_{-i}) = u_i(\sigma_i, \sigma_{-i}) \geq u_i(\sigma''_i. \sigma_{-i})$, para cualquier estrategia de $\sigma''_i$ del jugador $i$, por lo tanto $\sigma_i$ es mejor respuesta a $\sigma'_{-i}$ y $\sigma'_{i}$ es mejor respuesta a $\sigma_{i}$, para $i = 1, 2$ y por lo tanto $(\sigma'')$ y $\sigma'''$ también son equilibrios de Nash.
\end{proof}

\section*{Capítulo IV}

\begin{reptheorem}{theo:no-regret}
Sea $(s_t)_{t = 1, 2, ...}$ una secuencia de juegos de $\Gamma$.
Entonces, $R_i^t(j, k)$ converge a $0$ para cada $i$ y cada $j, k \in S_i$, con $j \neq k$, si y sólo si la secuencia de distribuciones empíricas $z_t$ converge al conjunto de equilibrio correlacionado.
\end{reptheorem}

\begin{proof}
Note que:
\begin{alignat}{1}
  D_i^t(j, k)\ 
    &=\ \frac{1}{t} \sum_{\substack{1\leq \tau \leq t \\ s_i^{\tau}=j}} u_i(k, s_{-i}^{\tau}) - u_i(s^{\tau}) \\
    &=\ \sum_{ \substack{s \in S \\ s_i = j}} \frac{1}{t} |\{1\leq\tau \leq t : s^{\tau} = s\}|\,[u_i(k, s_{-i}) - u_i(s)] \\
    &=\ \sum_{ \substack{s \in S \\ s_i = j}} z_t(s)\,[u_i(k, s_{-i}) - u_i(s)] \,.
\end{alignat}

Dado $\varepsilon > 0$, $R_i^t(j, k) \leq \varepsilon$ si y sólo si:
\begin{alignat}{1}
    \sum_{s \in S : s_i = j} z_t(s)\,[u_i(k, s_{-i}) - u_i(s)]\  =\ D_i^t(j, k)\ \leq \ \varepsilon \,,
\end{alignat}
obteniendo que $R_i^t(j, k) \leq \varepsilon$ para todo $i \in N$ y todo $j, k \in S_i$ si y sólo si $z_t$ es un $\varepsilon$-equilibrio correlacionado. Por lo tanto, todos los \textit{regrets} convergen a cero si y sólo si $z_t$ converge al conjunto de equilibrio correlacionado.
\end{proof}

\begin{reptheorem}{theo:def-proc-B}
Sea $R_t^i(j, j) = 0$. El vector $q_t^i$, definido en \ref{eq:def-inv-vector}, cumple que:
\begin{alignat}{1}
q^i_t(j)\sum_{k \in S_i} R^i_t(j,k)\ =\ \sum_{k \in S_i} q_t^i(k)R_i^t(k,j) \,.
\end{alignat}
\end{reptheorem}

\begin{proof}
\begin{alignat}{3}
  & & q^i_t(j)\ &=\ \biggl[\sum_{k \in S_i} q^i_t(k)\frac{1}{\mu}R^i_t(k,j)\biggr] + q^i_t(j)\biggl[1 - \sum_{k \in S_i} \frac{1}{\mu} R^i_t(j,k)\biggr] \\
  &\implies\
  &\mu q_t^i(j)\ &=\ \biggl[\sum_{k \in S_i}q^i_t(k)R^i_t(k,j)\biggr] + q^i_t(j)\biggl[\mu - \sum_{k \in S_i} R^i_t(j, k)\biggr] \\
  &\implies\
  &\mu q^i_t(j)\ & =\ \biggl[\sum_{k \in S_i}q^i_t(k)R^i_t(k,j)\biggr] + \mu q^i_t(j) - q^i_t(j)\sum_{k\in S_i} R^i_t(j,k) \,.
\end{alignat}

Luego,
\begin{alignat}{1}
q^i_t(j)\sum_{k \in S_i} R^i_t(j,k)\ =\ \sum_{k \in S_i} q_t^i(k)R_i^t(k,j) \,.
\end{alignat}
\end{proof}

\begin{reptheorem}{theo:conv-proc-B}
Supongamos que a cada período $t+1$, el jugador $i$ elige las estrategias acorde a un vector de distribución de probabilidad $q_t^i$ que satisface \eqref{eq:proc-B}. Entonces, $R^i_t(j, k)$ converge a cero (a. s.) para todo $j, k \in S_i$ con $j \neq k$.
\end{reptheorem}

\begin{proof}
La prueba es una aplicación directa del Teorema de Aproximación de Blackwell (Apéndice~\ref{apex:chapter:blackwell}) con $L$, $v$ y $\mathcal{C}$ definidos de la siguiente manera:
\begin{itemize}[noitemsep]
  \item $L = \{ (j, k) \in S_i \times S_{i}  : j \neq k \}$.
  \item $v(s_i, s_{-i}) \in \mathbb{R}^L$ dado por
    \begin{alignat}{1}
      [v(s_i, s_{-i})](j, k)\ =\  
        \begin{cases}
          u_i(k, s_{-i}) - u_i(j, s_{-i}) & \text{si } s_i = j \\
          0 & \text{en otro caso} \,.
        \end{cases}
    \end{alignat}
  \item $\mathcal{C} = \mathbb{R}^L_{-} = \{x \in \mathbb{R}^L : x_i \leq 0\ \forall i \in L \}$ es decir, el ortante negativo.
\end{itemize}

Se demostrará que $\mathcal{C}$ es alcanzable por $i$.
Note que:
\begin{alignat}{1}
	w_{\mathcal{C}}(\lambda)\ =\ \sup\{\lambda \cdot c : c \in \mathcal{C} \}\ =\ \sup \left \{ \sum_{i \in L} \lambda_i c_i : c_i \leq 0 \right \} \,.
\end{alignat}
Luego, si $\lambda_i \geq 0$, $\forall i \in L$, entonces $\lambda \cdot c \leq 0$ para todo $c \in \mathcal{C}$, y $w_{\mathcal{C}}(\lambda) = 0$. Por otra parte, si $\lambda_i < 0$ para algún $i\in N$, entonces $c_i \lambda_i$ no está acotado superiormente y  $w_{\mathcal{C}}(\lambda) = \infty$. Luego,
\begin{alignat}{1}
  w_{\mathcal{C}}\ =\  
	\begin{cases}
	  0 & \text{si } \lambda \in \mathbb{R}^L_+ \,, \\
	  \infty & \text{en caso contrario.}
	\end{cases}
\end{alignat}

Por otra parte, se tiene que:
\begin{alignat}{1}
	\lambda \cdot v(q_{\lambda}, s_{-i})\ 
	  &=\ \sum_{(j,k) \in L} \lambda(j,k) \cdot [v(q_{\lambda}, s_{-i})](j, k) \\
	&=\ \sum_{(j,k) \in L} \lambda(j, k)\left[\sum_{s_i \in S_i} q_{\lambda}(s_i) v(s_i, s_{-i}) \right](j, k) \\
	&=\ \sum_{(j,k) \in L} \lambda(j, k) q_{\lambda}(j) [v(j, s_{-i})](j, k) \\
	&=\ \sum_{(j,k) \in L} \lambda(j, k) q_{\lambda}(j) [u_i(k, s_{-i}) - u_i(j, s_{-i})] \\
	&=\ \sum_{(j,k) \in L} \lambda(j, k) q_{\lambda}(j)u_i(k, s_{-i}) - \sum_{(j,k) \in L} \lambda(j, k) q_{\lambda}(j)u_i(j, s_{-i}) \\
	&=\ \sum_{k \in S_i} u_i(k, s_{-i}) \sum_{j \in S_i} \lambda(j, k) q_{\lambda}(j) - \sum_{j \in S_i} q_{\lambda}(j)u_i(j, s_{-i}) \sum_{k \in S_i} \lambda(j, k) \\
	&=\ \sum_{j \in S_i} u_i(j, s_{-i}) \sum_{k \in S_i} \lambda(k, j) q_{\lambda}(k) - \sum_{j \in S_i} q_{\lambda}(j)u_i(j, s_{-i}) \sum_{k \in S_i} \lambda(j, k) \\
	&=\ \sum_{j \in S_i} u_i(j, s_{-i}) \left[ \sum_{k \in S_i} \lambda(k, j) q_{\lambda}(k) - q_{\lambda}(j) \sum_{k \in S_i} \lambda(j, k) \right] \,.
\end{alignat}

Se define:
\begin{alignat}{1}
  \alpha(j)\ =\ \sum_{k \in S_i} \lambda(k, j) q_{\lambda}(k) - q_{\lambda}(j) \sum_{k \in S_i} \lambda(j, k) \,,
\end{alignat}
entonces, $\lambda \cdot v(q_{\lambda}, s_{-i}) = \sum_{j \in S_i} u_i(j, s_{-i}) \alpha(j)$. Luego, la condición del Teorema \ref{theo:blackwell} es equivalente a:
\begin{alignat}{1}
	\sum_{j \in S_i} u_i(j, s_{-i}) \alpha(j)\ \leq\ 0 \,.
\end{alignat}

Si se elige $q_{\lambda}$ que cumpla 
\begin{alignat}{1}
  q_{\lambda}(j) \sum_{k \in S_i} \lambda(j, k)\ =\ \sum_{k \in S_i} \lambda(k, j) q_{\lambda}(k)
\end{alignat}
para todo $j \in S_i$, entonces $\alpha(j)=0$ para $j\in S_i$, y la condición del Teorema~\ref{theo:blackwell} se cumple como igualdad cuando $\mathcal{C} = \mathbb{R}^{L}_-$.

Por otra parte, sea $D_t=\frac{1}{t}\sum_{\tau=1}^t v(s_\tau)$ el promedio de los vectores de pago a tiempo $t$. Entonces,
\begin{alignat}{1}
  D_t[j, k]\ &=\ \sum_{\tau=1}^{t} v(s_{\tau})[j,k]\
	=\ \sum_{1\leq\tau \leq t, s_i^{\tau} = j} u_i(k, s_{-i}^{\tau}) - u_i(j, s_{-i}^{\tau})\
	=\ D_i^t(j, k) \,,
\end{alignat}
para $x \notin \mathbb{R}^-$, $F(x) = x^-$ y $\lambda (x) = x - x^- = x^+$, obteniendo 
\begin{alignat}{1}
	\lambda(D_t)\ =\ (R_t^i(j, k))_{(j, k) \in L} \,.
\end{alignat}

Luego, usar una estrategia que cumpla
\begin{alignat}{1}
	q_{\lambda}(j) \sum_{k \in S_i} \lambda(j, k)\ =\  \sum_{k \in S_i} q_{\lambda}(k) \lambda(k, j)
\end{alignat}
cuando $\lambda(j, k) = [D_i^t(j, k)]^+ = R_t^i(j, k)$ es equivalente que la estrategia $p_{t+1}^i \in \Delta(S_i)$ cumpla con
\begin{alignat}{1}
	p_{t+1}^i (j) \sum_{k \in S_i} R_i^t(j, k)\ =\ \sum_{k \in S_i} R_i^t(k, j) p_{t+1}^i(k) \,.
\end{alignat}

Aplicando el Teorema \ref{theo:blackwell} se tiene que al usar dicha estrategia, $D_t$ alcanza a $\mathbb{R}^-$ que es equivalente a que $R_i^t(j, k) \rightarrow 0$ para todo $j, k \in S_i$.
\end{proof}

\begin{reptheorem}{theo:conv-proc-C}
El procedimiento adaptativo definido en \eqref{eq:proc-C} es universalmente consistente para el jugador $i$.
\end{reptheorem}

\begin{proof}
La prueba es similar a la del procedimiento anterior. Se definen $L$, $v$ y $\mathcal{C}$ del Teorema \ref{theo:blackwell} de la siguiente manera:
\begin{itemize}[noitemsep]
  \item $L = S_i$.
  \item $v = v(s_i, s_{-i}) \in \mathbb{R}^L$ dada por:
    $[v(s_i, s_{-i})](k)=u_i(k, s_{-i}) - u_i(s_i, s_{-i})$.
  \item $\mathcal{C} = \mathbb{R}^L_- = \{x \in \mathbb{R}^L : x_i \leq 0\ \forall i \in L \}$ (i.e.\ el ortante negativo).
\end{itemize}

Se demostrará que $\mathcal{C}$ es alcanzable por $i$. Al igual que antes, se tiene que:
\begin{alignat}{1}
  w_{\mathcal{C}}\ =\ 
    \begin{cases}
	  0 & \text{si } \lambda \in \mathbb{R}^L_+ \,, \\
      \infty & \text{en caso contrario.}
    \end{cases}
\end{alignat}

Por otra parte,
\begin{alignat}{1}
	\lambda \cdot v(q_{\lambda}, s_{-i})\ 
	&=\ \sum_{k \in L} \lambda(k) \cdot [v(q_{\lambda}, s_{-i})](k) \\
	&=\ \sum_{k \in S_i} \lambda(k) \cdot \sum_{j \in S_i} q_{\lambda}(j)[v(j, s_{-i})] (k) \\
	&=\ \sum_{k \in S_i} \lambda(k) \cdot \sum_{j \in S_i} q_{\lambda}(j) [u_i(k, s_{-i}) - u_i(j, s_{-i})] \\
	&=\ \sum_{\substack{k \in S_i \\ j \in S_i}} \lambda(k) q_{\lambda}(j) [u_i(k, s_{-i}) - u_i(j, s_{-i})] \\
	&=\ \sum_{\substack{k \in S_i \\ j \in S_i}} \lambda(k) q_{\lambda}(j) u_i(k, s_{-i}) - \sum_{\substack{k \in S_i \\ j \in S_i}} \lambda(k) q_{\lambda}(j) u_i(j, s_{-i}) \\
	&=\ \sum_{\substack{j \in S_i \\ k \in S_i}} u_i(j, s_{-i}) \lambda(j) q_{\lambda}(k)  - \sum_{\substack{j \in S_i \\ j \in S_i}} u_i(j, s_{-i}) \lambda(k) q_{\lambda}(j) \\
	&=\ \sum_{\substack{j \in S_i \\ k \in S_i}} u_i(j, s_{-i}) [\lambda(j) q_{\lambda}(k)  - \lambda(k) q_{\lambda}(j)] \\
	&=\ \sum_{j \in S_i} u_i(j, s_{-i}) \left[ \lambda(j) \sum_{k \in S_i} q_{\lambda}(k)  - q_{\lambda}(j) \sum_{k \in S_i} \lambda(k)  \right] \\
	&=\ \sum_{j \in S_i} u_i(j, s_{-i}) \left[ \lambda(j) - q_{\lambda}(j) \sum_{k \in S_i} \lambda(k) \right] \,.
\end{alignat}

La última igualdad ocurre porque $\sum_{k\in S_i} q_{\lambda}(k)=1$. Luego, si se define:
\begin{alignat}{1}
	\alpha(j)\ =\ \lambda(j) - q_{\lambda}(j) \sum_{k \in S_i} \lambda(k) \,,
\end{alignat}
se obtiene $\lambda \cdot v(q_{\lambda}, s_{-i}) = \sum_{j \in S_i} u_i(j, s_{-i})\alpha(j)$.
Note que si $q_{\lambda(j)} = \frac{\lambda(j)}{\sum_{k \in S_i} \lambda(k)}$, entonces $\alpha(j) = 0$ para todo $j \in S_i$ y se cumple la condición del Teorema \ref{theo:blackwell} en forma de igualdad. Además, para $D_t=\frac{1}{t} \sum_{\tau = 1}^{t} v(s_{\tau})$, se tiene
\begin{alignat}{1}
	D_t[k]\ =\ \sum_{\tau = 1}^{t} v(s_{\tau})[k]\ =\ \sum_{\tau \leq t }[u_i(k, s_{-i}^{\tau}) - u_i(s_{\tau})]\ =\  D_i^t(k) \,.
\end{alignat}

Luego $F(D_t) = D_t^-$ y $\lambda(D_t) = D_t^+ = (R_i^t(k))_{k \in S_i}$, obteniendo:
\begin{alignat}{1}
	q_{\lambda(D_t)}\ =\ \frac{[\lambda(D_t)](j)}{\sum_{k \in S_i}[\lambda(D_t)](k)}\ =\ \frac{R_i^t(j)}{\sum_{k \in S_i} R_i^t(k)} \,.
\end{alignat}

Al elegir $p_{t+1}(j) = q_{\lambda(D_t)}(j) = \frac{R_i^t (j)}{\sum_{k \in S_i} R_i^t(k)}$, se obtiene que $D_t$ alcanza a $\mathbb{R^-}$, lo cual es equivalente a que $R_i^t(j) \rightarrow 0$ para todo $j \in S_i$.
\end{proof}


\begin{reptheorem}{theo:UC-EN}
Sea $\Gamma$ un juego de dos jugadores de suma cero y sea $(s^t)_{t=1,2,..., T}$ una secuencia de juegos de $\Gamma$, tales que, para todo $s_i \in S_i$, para todo $i \in {1, 2}$:
\begin{alignat}{1}
\frac{1}{T}\sum_{t = 1}^{T}u_i(s_i, s_{-i}^t) - \frac{1}{T} \sum_{t = 1}^T u_i(s^t)\ \leq\ \varepsilon
\end{alignat}
para algún $\varepsilon > 0$. Sea $\bar{\sigma}^T = (\bar{\sigma_1}^T, \bar{\sigma_2}^T)$, donde:
\begin{alignat}{1}
\bar{\sigma}_i^T(s_i)\ =\ \frac{|\{ t \leq T : s_i^t = s_i\}|}{T} \,,
\end{alignat}
es decir, $\bar{\sigma}^T$ es la distribución empírica de probabilidad, note que $|\{ t \leq T : s_i^t = s_i\}|$ es igual al número de veces que se eligió $s_i$ hasta el tiempo $T$. Entonces, $\bar{\sigma}^T$ es un $2\varepsilon$-equilibrio de Nash.
\end{reptheorem}

\begin{proof}
Se denotará con $\#(s_i)$ el número de veces que se ha elegido $s_i$ a tiempo $T$, i.e., $\#(s_i) = |\{ t \leq T : s_i^t = s_i\}|$. Por hipótesis del teorema, se tiene que:
\begin{alignat}{1}
\frac{1}{T} \sum_{t = 1}^T u_i(s_i, s_{-i}^t) - \frac{1}{T} \sum_{t = 1}^T u_i(s^t)\ \leq\ \varepsilon \,.
\end{alignat}

Reordenado la sumatoria del primer término y utilizando la definición de $\bar\sigma$, se obtiene:
\begin{alignat}{3}
& \frac{1}{T} \sum_{s_{-i} \in S_{-i}} \#(s_{-i})u_i(s_i, s_{-i}) - \frac{1}{T} \sum_{t = 1}^Tu_i(s^t)\ & \leq\ & \ \varepsilon \\
\implies\ & \sum_{s_{-i} \in S_{-i}} \bar{\sigma}_{-i}^T(s_{-i})u_i(s_i, s_{-i}) - \frac{1}{T} \sum_{t = 1}^T u_i(s^t)\ & \leq\ & \ \varepsilon \,.
\end{alignat}

Sea $\sigma_i \in \Delta(S_i)$ cualquier estrategia del jugador $i$, luego:
\begin{alignat}{4}
& & \sum_{s_i \in S_i} \sigma_i(s_i) \left[ \sum_{s_{-i} \in S_{-i}} \bar{\sigma}_{-i}^T(s_{-i})u_i(s_i, s_{-i}) - \frac{1}{T} \sum_{t = 1}^T u_i(s^t) \right]\ & \leq\  & & \ \sum_{s_i \in S_i} \sigma_i(s_i) \varepsilon \\
& \implies & \sum_{s_i \in S_i} \sum_{s_{-i} \in S_{-i}} \sigma_i(s_i)\bar{\sigma}_{-i}^T(s_{-i}) u_i(s_i, s_{-i}) - \sum_{s_i \in S_i} \sigma_i(s_i)u_i(s^t)\ & \leq\  & & \ \varepsilon \\
& \implies & u_i(\sigma_i, \bar{\sigma}_{-i}^T) - \frac{1}{T} \sum_{t = 1}^T u_i(s^t)\ & \leq\ &  & \ \varepsilon \,.
\end{alignat}

En particular, se tiene que para estrategias cualesquiera $\sigma_1 \in \Delta(S_1)$ y $\sigma_2 \in \Delta(S_2)$
\begin{alignat}{1}
\label{eq:star1}
u_1(\sigma_1, \bar{\sigma}_2^T) - \frac{1}{T} \sum_{t=1}^T u_1(s^t)\ \leq\ \varepsilon \\
u_2(\bar{\sigma}_1^T, \sigma_2) - \frac{1}{T} \sum_{t=1}^T u_2(s^t)\ \leq\ \varepsilon \,.
\end{alignat}

Además, como $\Gamma$ es un juego de suma cero, se tiene que $u_2(\bar{\sigma}_1^T, \sigma_2) = -u_1(\bar{\sigma}_1^T, \sigma_2)$ y $u_2(s^t) = -u_1(s^t)$, luego:
\begin{alignat}{1}
u_2(\bar{\sigma}_1^T, \sigma_2) - \frac{1}{T} \sum_{t=1}^T u_2(s^t)\ =\ -u_1(\bar{\sigma}_1^T, \sigma_2) - \frac{1}{T} \sum_{t=1}^T -u_1(s^t)\ \leq\ \varepsilon \,.
\end{alignat}

En particular, si $\sigma_2 = \bar{\sigma_2}^T$ entonces:
\begin{alignat}{1}
\label{eq:star2}
-u_1(\bar{\sigma}_1^T, \bar{\sigma_2}^T) + \frac{1}{T} \sum_{t=1}^T u_1(s^t)\ \leq\ \varepsilon \,.
\end{alignat}

Al sumar las desigualdades \ref{eq:star1} y \ref{eq:star2} se obtiene que:
\begin{alignat}{2}
& u_1(\sigma_1, \bar{\sigma}_2^T) - u_1(\bar{\sigma}_1^T, \bar{\sigma}_2^T)\ \leq\ 2\varepsilon \\
\implies\ & u_1(\bar{\sigma}^T) + 2\varepsilon\ \geq\ u_1(\sigma_1, \bar{\sigma}_2^T) \,.
\end{alignat}

Análogamente se tiene que $u_2(\bar{\sigma}^T) + 2\varepsilon \geq u_2(\bar{\sigma_1}^T, \sigma_2)$, con lo que se concluye que $\bar{\sigma}^T$ es un $2\varepsilon$-equilibrio de Nash.
\end{proof}

\begin{reptheorem}{theo:a-b-universalmente-consistentes}
En un procedimiento adaptativo de \textit{Regret Matching}, si el \textit{regret} condicional converge a $0$, entonces el procedimiento es universalmente consistente.
\end{reptheorem}

\begin{proof}
Se demostrará, como en el teorema anterior, que el \textit{regret} incondicional tiende a $0$. De la Ecuación~\ref{eq:diferencia-pago} se tiene que:
\begin{alignat}{1}
  D_i^t(j, k)\ =\ \frac{1}{t} \sum_{\substack{1\leq \tau \leq t \\s^\tau_i = j}} u_i(k, s_{-i}^{\tau}) - u_i(s^{\tau})\,
\end{alignat}
si se suman los $D_i^t(j, k)$ sobre $j \in S_i$, se obtiene:
\begin{alignat}{1}
  \sum_{j \in S_i} D_i^t(j, k)\ =\ \sum_{j \in S_i}{\left( \frac{1}{t} \sum_{\substack{1\leq \tau \leq t \\s^\tau_i = j}} u_i(k, s_{-i}^{\tau}) - u_i(s^{\tau}) \right)}\ =\ \frac{1}{t} \sum_{1\leq \tau \leq t} u_i(k, s_{-i}^{\tau}) - u_i(s^{\tau}) \,.
\end{alignat}
De la Ecuación~\ref{eq:diferencia-pago-ri} se obtiene el último miembro de la igualdad es igual a $D_i^t(k)$. Luego:
\begin{alignat}{1}
    \sum_{j \in S_i} D_i^t(j, k)\ =\ D_i^t(k) \,.
\end{alignat}

Por otra parte, utilizando la desigualdad triangular en $\mathbb{R}$, se tiene que
\begin{alignat}{1}
    \sum_{j \in S_i} \max \left\{ 0, D^t_i(j, k)\right\} \geq \max\left\{0, \sum_{j \in S_i} D^t_i(j, k)\right\},
\end{alignat}
al sustituir por las definiciones de $R_i^t(j, k)$ y $R_i^t(k)$ se concluye que:
\begin{alignat}{1}
    \sum_{j \in S_i} R_i^t(j, k) \geq R_i^t(k) \,.
\end{alignat}

Luego como $R_i^t(j, k) \rightarrow 0$ cuando $t \rightarrow \infty$ para todo $j, k \in S_i$, entonces $\sum_{j \in S_i} R_i^t(j, k) \rightarrow 0$ cuando $t \rightarrow \infty$ y por lo tanto $R_i^t(k) \rightarrow 0$ cuando $t \rightarrow \infty$. Luego si el \textit{regret} condicional converge a 0, entonces el \textit{regret} incondicional también converge a $0$ y por lo tanto el procedimiento es universalmente consistente.
\end{proof}

\section*{Capítulo V}

\begin{reptheorem}{theo:regret-nash}
En un juego de $2$ jugadores de suma cero si el \textit{regret} promedio general a tiempo $T$ es menor que $\varepsilon$ entonces $\sigma^{-T}$ es un $2\varepsilon$-equilibrio de Nash
\end{reptheorem}

\begin{proof}
Se probará que la probabilidad de alcanzar $z$ bajo $\bar{\sigma}_i^T$ viene dada por el promedio de alcanzar $z$ en  cada estrategia. Sean $h_1 \sqsubset h_2 \sqsubset h_3 \sqsubset ... \sqsubset h_m \sqsubset z$ todos los prefijos de $z$ correspondientes al jugador $i$, es decir $P(h_k) = i\ \forall k : 1 \leq k \leq m$ y sean $a_1, a_2, ..., a_m$ las acciones correspondientes en $z$ en cada historia respectiva. Luego:
\begin{alignat}{2}
\pi^{\bar{\sigma}_i^T}(z)\ &=\ \prod_{k = 1}^m \bar{\sigma}_i^T(I(h_k))(a_k) \\	&=\ \prod_{k = 1}^m \frac{\sum_{t = 1}^{T}  \pi^{\sigma_i^t}(I(h_k))\sigma^t_i(I(h_k))(a)}{\sum_{t = 1}^T \pi^{\sigma_i^t}(I(h_k))}
\end{alignat}

Por otra parte, note que $\pi^{\sigma_i^t}(I)\sigma_i^t(I(h_k))(a_k) = \pi^{\sigma_i^t}(I(h_{k+1}))$. Entonces:
\begin{alignat}{1}
	\pi^{\bar{\sigma}_i^T}(z)\ &=\ \frac{\sum_{t = 1}^T\pi^{\sigma_i^t}(I_m) \sigma_i^t(I_m)(a_m)}{\sum_{t = 1}^T \pi^{\sigma_i^t}(I_1)} \\
	&=\ \frac{\sum_{t = 1}^T \pi^{\sigma_i^t}(z)}{\sum_{t = 1}^T 1} \\
	&=\ \frac{1}{T} \sum_{t = 1}^T \pi^{\sigma_i^t}(z) \,.
\end{alignat}

Además, se tiene que, para cualquier jugador $i$ y cualquier estrategia de $\sigma_i$:
\begin{alignat}{2}
\frac{1}{T} \sum_{t = 1}^T u_i(\sigma'_i, \sigma_{-i}^t)\ &=\ \frac{1}{T} \sum_{t = 1}^T \left( \sum_{z \in Z} \pi^{\sigma'_i}(z) \pi^{\sigma_{-i}^t}(z) \pi^c(z) \right) \\
	&=\ \sum_{z \in Z} u_i(z) \pi^{\sigma'_i}(z) \pi^c(z) \left( \frac{1}{T} \sum_{t = 1}^T \pi^{\sigma_{-i}^t}(z) \right) \\
	&=\ \sum_{z \in Z} u_i(z) \pi^{\sigma'_i}(z) \pi^{\bar{\sigma}_i^T}(z) \pi^c(z)\ =\ u_i(\sigma'_i, \bar{\sigma}_{-i}^T) \,.
\end{alignat}

Por otra parte, como $R_2^T \leq \varepsilon$, para todo $\sigma'_2 \in B_2$ se tiene que:
\begin{alignat}{2}
	& \frac{1}{T} \sum_{t = 1}^T[u_2(\sigma_1^t, \sigma'_2) - u_2(\sigma^t)]\  \leq\  \varepsilon\\
	\implies\ & \frac{1}{T} \sum_{t = 1}^T u_2(\sigma^t) + \varepsilon\  \geq\  \frac{1}{T} \sum_{t = 1}^T u_2(\sigma_1^t, \sigma'_2) \\
	\implies\ & \frac{1}{T} \sum_{t = 1}^T u_2(\sigma^t) + \varepsilon\ \geq\ u_2(\bar{\sigma}_1^T, \sigma'_2) \,.
\end{alignat}

Como se cumple para cualquier estrategia $\sigma'_2$, se cumple para $\sigma_2^t$ para $t = 1, 2, ..., T$, obteniendo:
\begin{alignat}{2}
\frac{1}{T} \sum_{t = 1}^T u_2(\sigma^t) + \varepsilon\ & \geq\ \frac{1}{T} \sum_{t = 1}^T u_2(\bar{\sigma}_1^T, \sigma_2^t) \\
&=\ \frac{1}{T} \sum_{t = 1}^T \sum_{z \in Z} u_2(z) \pi^{\bar{\sigma}_1^T}\pi^{\sigma_2^t}(z) \pi^c(z) \\
&=\  \sum_{z \in Z} u_2(z) \pi^{\bar{\sigma}_1^T} \pi^c(z) \left( \frac{1}{T} \sum_{t = 1}^T \pi^{\sigma_2^t}(z) \right)\\
&=\  \sum_{z \in Z} u_2(z) \pi^{\bar{\sigma}_1^T} \pi^{\bar{\sigma}_2^T} \pi^c(z) \\
&=\ u_2(\bar{\sigma}^T) \,.
\end{alignat}

Como $\Gamma$ es un juego de suma cero, se tiene que $u_2(\sigma) = -u_1(\sigma)$ para cualquier estrategia $\sigma$, luego:
\begin{alignat}{2}
	& \frac{1}{T} \sum_{t = 1}^T u_2(\sigma^t) + \varepsilon\ \geq\  u_2(\bar{\sigma}^T) \\
	\implies\ & \frac{1}{T} \sum_{t = 1}^T -u_1(\sigma^t) + \varepsilon\ \geq\  -u_1(\bar{\sigma}^T) \\
	\implies\ &  u_1(\bar{\sigma}^T) + \varepsilon\ \geq\   \frac{1}{T} \sum_{t = 1}^T u_1(\sigma^t)  \\
	\implies\ &  u_1(\bar{\sigma}^T) + 2\varepsilon\ \geq\   \frac{1}{T} \sum_{t = 1}^T u_1(\sigma^t) + \varepsilon
\end{alignat}

Por otra parte, como $R_i^t \leq \varepsilon$ se tiene que, para cualquier $\sigma'_1 \in B_1$:
\begin{alignat}{1}
	\frac{1}{T} \sum_{t = 1}^T u_1(\sigma^t) + \varepsilon\ \geq\ \frac{1}{T} \sum_{t = 1} u_1(\sigma'_1, \sigma_2^t)\ =\ u_1(\sigma'_1, \bar{\sigma}_2^T) \,.
\end{alignat}

Luego, se obtiene que:
\begin{alignat}{1}
	& u_1(\bar{\sigma}^T) + 2\varepsilon\ \geq\   \frac{1}{T} \sum_{t = 1}^T u_1(\sigma^t) + \varepsilon\ \geq\ u_1(\sigma'_1, \bar{\sigma}_2^T)\\
	\implies\ & u_1(\bar{\sigma}^T) + 2\varepsilon\ \geq\ u_1(\sigma'_1, \bar{\sigma}_2^T) \,.
\end{alignat}

Análogamente, se demuestra que $u_2(\bar{\sigma}^T)\ + 2\varepsilon\ \geq\ u_2( \bar{\sigma}_1^T, \sigma'_2)$ concluyendo que $\bar{\sigma}^T$ es un $2\varepsilon$-equilibrio de Nash.
\end{proof}

\begin{reptheorem}{theo:esperanza-MCCFR}
$E_{j \sim q_j} [\tilde{u}_i(\sigma, I | j)]\ =\ u_i(\sigma, I)$
\end{reptheorem}

\begin{proof}
\begin{alignat}{2}
    E_{j \sim q_j}[\tilde{u}_i(\sigma, I | j)]\ & =\ \sum_{j} {q_j u_i(\sigma, I)} \\
    & =\ \sum_{j} { q_j \frac{\sum_{h \in I, z \in Q_j} \pi^{\sigma_{-i}}(h) \pi^{\sigma}(h, z) u_i(z)}{q(z) \pi^{\sigma_{-i}}(I)}} \\ \label{eq:def-esperanza}
    & =\ \sum_{j} \sum_{ \substack{h \in I \\ z \in Q_j}} \frac{q_j \pi^{\sigma_{-i}}(h) \pi^{\sigma}(h, z) u_i(z)}{ q(z) \pi^{\sigma_{-i}}(I)} \\ \label{eq:reorder-sum-1}
    & =\ \sum_{ \substack{h \in I \\ z \in Z} } \sum_{j | z \in Q_j} \frac{q_j \pi^{\sigma_{-i}}(h) \pi^{\sigma}(h, z) u_i(z)}{ q(z) \pi^{\sigma_{-i}}(I)} \\ \label{eq:reorder-sum-2}
    & =\ \sum_{ \substack{h \in I \\ z \in Z} } \left(\frac{\sum_{j | z \in Q_j} q_j }{q(z)}\right) \frac{\pi^{\sigma_{-i}}(h) \pi^{\sigma}(h, z) u_i(z)}{\pi^{\sigma_{-i}}(I)} \\
    & =\ \sum_{ \substack{h \in I \\ z \in Z} } \frac{\pi^{\sigma_{-i}}(h) \pi^{\sigma}(h, z) u_i(z)}{\pi^{\sigma_{-i}}(I)}  = u_i(\sigma, I) \label{eq:lemma-final-eq}
\end{alignat}
La ecuación \ref{eq:def-esperanza} se obtiene de la definición de $\tilde{u}_i(\sigma, I | j)$. \ref{eq:reorder-sum-1} y \ref{eq:reorder-sum-2} se obtienen al reordenar las sumatorias y considerando que la unión de los bloques generan a $Z$. La ecuación \ref{eq:lemma-final-eq} es la definición de utilidad contrafactual.
\end{proof}
